The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `8`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-03-13:16:34:58,217 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-03-13:16:34:58,217 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-03-13:16:34:58,217 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-03-13:16:34:58,217 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-03-13:16:34:58,217 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-13:16:34:58,217 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-13:16:34:58,217 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-13:16:34:58,217 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-03-13:16:34:58,217 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-13:16:34:58,217 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-03-13:16:34:58,217 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-13:16:34:58,217 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-03-13:16:34:58,217 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-13:16:34:58,217 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-13:16:34:58,217 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-03-13:16:34:58,217 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-13:16:35:00,409 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-03-13:16:35:00,412 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-03-13:16:35:00,412 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-03-13:16:35:00,412 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-03-13:16:35:00,413 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-03-13:16:35:00,413 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-03-13:16:35:00,413 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-03-13:16:35:00,413 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-03-13:16:35:12,609 INFO     [__main__.py:156] Verbosity set to INFO
2024-03-13:16:35:12,609 INFO     [__main__.py:156] Verbosity set to INFO
2024-03-13:16:35:12,609 INFO     [__main__.py:156] Verbosity set to INFO
2024-03-13:16:35:12,609 INFO     [__main__.py:156] Verbosity set to INFO
2024-03-13:16:35:12,609 INFO     [__main__.py:156] Verbosity set to INFO
2024-03-13:16:35:12,609 INFO     [__main__.py:156] Verbosity set to INFO
2024-03-13:16:35:12,609 INFO     [__main__.py:156] Verbosity set to INFO
2024-03-13:16:35:12,610 INFO     [__main__.py:156] Verbosity set to INFO
2024-03-13:16:35:29,663 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-03-13:16:35:29,663 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-03-13:16:35:29,663 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-03-13:16:35:29,663 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-03-13:16:35:29,664 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-03-13:16:35:29,664 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-03-13:16:35:29,664 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-03-13:16:35:29,664 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-03-13:16:36:51,279 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-03-13:16:36:51,289 WARNING  [__main__.py:213] File already exists at files/model_evals/pythia-1b-helpful-sfted1-ppo-ep1-0shot. Results will be overwritten.
2024-03-13:16:36:51,289 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
2024-03-13:16:36:51,544 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-03-13:16:36:51,548 WARNING  [__main__.py:213] File already exists at files/model_evals/pythia-1b-helpful-sfted1-ppo-ep1-0shot. Results will be overwritten.
2024-03-13:16:36:51,548 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
2024-03-13:16:36:51,595 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-03-13:16:36:51,599 WARNING  [__main__.py:213] File already exists at files/model_evals/pythia-1b-helpful-sfted1-ppo-ep1-0shot. Results will be overwritten.
2024-03-13:16:36:51,600 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
2024-03-13:16:36:51,829 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-03-13:16:36:51,833 WARNING  [__main__.py:213] File already exists at files/model_evals/pythia-1b-helpful-sfted1-ppo-ep1-0shot. Results will be overwritten.
2024-03-13:16:36:51,833 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
2024-03-13:16:36:52,221 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-03-13:16:36:52,224 WARNING  [__main__.py:213] File already exists at files/model_evals/pythia-1b-helpful-sfted1-ppo-ep1-0shot. Results will be overwritten.
2024-03-13:16:36:52,225 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
2024-03-13:16:36:52,236 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-03-13:16:36:52,241 WARNING  [__main__.py:213] File already exists at files/model_evals/pythia-1b-helpful-sfted1-ppo-ep1-0shot. Results will be overwritten.
2024-03-13:16:36:52,241 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
2024-03-13:16:36:52,993 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-03-13:16:36:52,997 WARNING  [__main__.py:213] File already exists at files/model_evals/pythia-1b-helpful-sfted1-ppo-ep1-0shot. Results will be overwritten.
2024-03-13:16:36:52,997 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
2024-03-13:16:36:54,121 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-03-13:16:36:54,125 WARNING  [__main__.py:213] File already exists at files/model_evals/pythia-1b-helpful-sfted1-ppo-ep1-0shot. Results will be overwritten.
2024-03-13:16:36:54,125 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
Some weights of the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs were not used when initializing GPTNeoXForCausalLM: ['frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.0.attention.query_key_value.bias', 'frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.4.attention.query_key_value.weight', 'base_model.gpt_neox.layers.8.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.9.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.11.attention.query_key_value.weight', 'base_model.gpt_neox.layers.9.attention.dense.weight', 'base_model.gpt_neox.layers.9.attention.dense.bias', 'frozen_head.decoder_blocks.1.input_layernorm.weight', 'base_model.gpt_neox.layers.13.input_layernorm.weight', 'frozen_head.decoder_blocks.0.attention.dense.weight', 'base_model.gpt_neox.layers.11.attention.query_key_value.bias', 'base_model.gpt_neox.layers.7.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.6.attention.dense.weight', 'base_model.gpt_neox.layers.4.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.14.attention.query_key_value.bias', 'v_head.2.bias', 'base_model.gpt_neox.layers.2.input_layernorm.weight', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.2.attention.dense.weight', 'base_model.gpt_neox.layers.3.attention.query_key_value.weight', 'base_model.gpt_neox.layers.8.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.11.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.3.attention.dense.bias', 'base_model.gpt_neox.layers.14.attention.query_key_value.weight', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.10.attention.query_key_value.bias', 'base_model.gpt_neox.layers.1.attention.dense.bias', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.0.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.1.attention.dense.weight', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.15.attention.query_key_value.bias', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.0.input_layernorm.bias', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.10.attention.dense.weight', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.5.post_attention_layernorm.weight', 'frozen_head.lm_head.weight', 'base_model.gpt_neox.layers.1.attention.query_key_value.bias', 'base_model.gpt_neox.layers.13.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.4.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.0.attention.dense.bias', 'frozen_head.decoder_blocks.2.attention.dense.weight', 'frozen_head.decoder_blocks.2.post_attention_layernorm.weight', 'base_model.embed_out.weight', 'base_model.gpt_neox.layers.2.attention.dense.bias', 'base_model.gpt_neox.layers.3.input_layernorm.weight', 'base_model.gpt_neox.layers.7.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.3.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.15.input_layernorm.bias', 'base_model.gpt_neox.layers.2.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.3.input_layernorm.weight', 'base_model.gpt_neox.layers.12.input_layernorm.bias', 'base_model.gpt_neox.layers.10.attention.query_key_value.weight', 'base_model.gpt_neox.final_layer_norm.bias', 'frozen_head.decoder_blocks.1.attention.query_key_value.bias', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.7.input_layernorm.weight', 'base_model.gpt_neox.layers.13.attention.query_key_value.weight', 'base_model.gpt_neox.layers.6.attention.dense.bias', 'frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.11.attention.dense.weight', 'base_model.gpt_neox.layers.3.attention.query_key_value.bias', 'base_model.gpt_neox.layers.11.attention.dense.bias', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.15.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.0.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.6.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.0.post_attention_layernorm.bias', 'v_head.0.weight', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.5.input_layernorm.weight', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.10.attention.dense.bias', 'base_model.gpt_neox.layers.8.attention.dense.weight', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.1.attention.query_key_value.weight', 'base_model.gpt_neox.layers.2.input_layernorm.bias', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.7.attention.dense.weight', 'base_model.gpt_neox.layers.14.input_layernorm.weight', 'base_model.gpt_neox.layers.4.attention.dense.bias', 'base_model.gpt_neox.layers.7.attention.dense.bias', 'base_model.gpt_neox.layers.9.attention.query_key_value.bias', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.1.attention.dense.weight', 'base_model.gpt_neox.layers.6.input_layernorm.bias', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.15.attention.query_key_value.weight', 'base_model.gpt_neox.layers.1.input_layernorm.weight', 'base_model.gpt_neox.layers.11.input_layernorm.bias', 'base_model.gpt_neox.layers.1.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.2.attention.query_key_value.bias', 'base_model.gpt_neox.layers.9.attention.query_key_value.weight', 'frozen_head.decoder_blocks.2.attention.dense.bias', 'base_model.gpt_neox.layers.12.input_layernorm.weight', 'frozen_head.decoder_blocks.1.input_layernorm.bias', 'base_model.gpt_neox.embed_in.weight', 'base_model.gpt_neox.layers.3.input_layernorm.bias', 'frozen_head.decoder_blocks.3.input_layernorm.bias', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.2.attention.query_key_value.weight', 'base_model.gpt_neox.layers.7.input_layernorm.bias', 'base_model.gpt_neox.layers.14.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.11.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.15.input_layernorm.weight', 'base_model.gpt_neox.layers.5.attention.query_key_value.bias', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.2.input_layernorm.weight', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.1.input_layernorm.bias', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.0.attention.query_key_value.weight', 'base_model.gpt_neox.layers.12.attention.query_key_value.bias', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.3.attention.dense.weight', 'base_model.gpt_neox.layers.14.attention.dense.weight', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.0.attention.dense.weight', 'frozen_head.decoder_blocks.2.attention.query_key_value.weight', 'base_model.gpt_neox.layers.15.attention.dense.bias', 'base_model.gpt_neox.layers.5.attention.query_key_value.weight', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.13.input_layernorm.bias', 'frozen_head.decoder_blocks.1.attention.dense.bias', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.2.input_layernorm.bias', 'frozen_head.decoder_blocks.0.input_layernorm.bias', 'base_model.gpt_neox.layers.7.attention.query_key_value.weight', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.0.attention.query_key_value.bias', 'frozen_head.final_norm.bias', 'base_model.gpt_neox.layers.15.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.10.input_layernorm.weight', 'base_model.gpt_neox.layers.1.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.1.attention.query_key_value.weight', 'base_model.gpt_neox.layers.5.attention.dense.weight', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.2.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.0.attention.dense.bias', 'base_model.gpt_neox.layers.4.input_layernorm.weight', 'frozen_head.decoder_blocks.3.attention.query_key_value.weight', 'base_model.gpt_neox.layers.13.attention.dense.bias', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.12.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.5.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.9.input_layernorm.weight', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.14.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.13.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.5.attention.dense.bias', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.11.input_layernorm.weight', 'frozen_head.decoder_blocks.0.input_layernorm.weight', 'frozen_head.decoder_blocks.3.attention.query_key_value.bias', 'frozen_head.final_norm.weight', 'base_model.gpt_neox.layers.10.input_layernorm.bias', 'base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.12.attention.dense.weight', 'base_model.gpt_neox.layers.4.attention.query_key_value.bias', 'frozen_head.decoder_blocks.2.attention.query_key_value.bias', 'base_model.gpt_neox.layers.12.attention.query_key_value.weight', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.7.attention.query_key_value.bias', 'frozen_head.decoder_blocks.1.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.13.attention.query_key_value.bias', 'base_model.gpt_neox.layers.3.attention.dense.bias', 'base_model.gpt_neox.layers.12.attention.dense.bias', 'base_model.gpt_neox.layers.8.attention.dense.bias', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.8.input_layernorm.bias', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.14.input_layernorm.bias', 'frozen_head.decoder_blocks.2.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.0.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.bias', 'v_head.0.bias', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.final_layer_norm.weight', 'base_model.gpt_neox.layers.3.attention.dense.weight', 'base_model.gpt_neox.layers.13.attention.dense.weight', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.0.attention.query_key_value.weight', 'base_model.gpt_neox.layers.10.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.5.input_layernorm.bias', 'base_model.gpt_neox.layers.6.attention.query_key_value.bias', 'base_model.gpt_neox.layers.8.attention.query_key_value.bias', 'frozen_head.decoder_blocks.1.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.15.attention.dense.weight', 'base_model.gpt_neox.layers.14.attention.dense.bias', 'base_model.gpt_neox.layers.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.6.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.10.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.4.attention.dense.weight', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.bias', 'v_head.2.weight', 'base_model.gpt_neox.layers.12.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.0.input_layernorm.weight', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.6.attention.query_key_value.weight', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.8.attention.query_key_value.weight', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.9.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.9.input_layernorm.bias', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.4.input_layernorm.bias', 'base_model.gpt_neox.layers.8.input_layernorm.weight', 'base_model.gpt_neox.layers.6.input_layernorm.weight']
- This IS expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs and are newly initialized: ['layers.3.post_attention_layernorm.bias', 'layers.7.mlp.dense_4h_to_h.bias', 'layers.11.attention.query_key_value.weight', 'layers.15.input_layernorm.weight', 'layers.9.input_layernorm.bias', 'layers.1.attention.dense.bias', 'layers.4.post_attention_layernorm.bias', 'layers.7.mlp.dense_h_to_4h.weight', 'layers.3.mlp.dense_4h_to_h.weight', 'layers.0.attention.dense.bias', 'layers.0.post_attention_layernorm.bias', 'layers.10.attention.dense.weight', 'layers.15.post_attention_layernorm.bias', 'embed_out.weight', 'layers.2.post_attention_layernorm.bias', 'layers.10.mlp.dense_4h_to_h.bias', 'layers.9.post_attention_layernorm.bias', 'embed_in.weight', 'layers.11.attention.dense.bias', 'layers.8.input_layernorm.bias', 'layers.6.input_layernorm.bias', 'layers.6.mlp.dense_h_to_4h.bias', 'layers.15.mlp.dense_4h_to_h.weight', 'layers.13.post_attention_layernorm.weight', 'layers.0.attention.dense.weight', 'layers.8.post_attention_layernorm.weight', 'layers.12.mlp.dense_h_to_4h.bias', 'layers.10.attention.query_key_value.bias', 'final_layer_norm.bias', 'layers.14.mlp.dense_4h_to_h.weight', 'layers.6.mlp.dense_4h_to_h.bias', 'layers.1.attention.query_key_value.weight', 'layers.0.input_layernorm.weight', 'layers.2.input_layernorm.bias', 'layers.11.mlp.dense_h_to_4h.weight', 'layers.9.attention.dense.bias', 'layers.14.input_layernorm.bias', 'layers.1.mlp.dense_h_to_4h.bias', 'layers.7.attention.query_key_value.weight', 'layers.15.attention.dense.weight', 'layers.7.input_layernorm.weight', 'layers.6.attention.query_key_value.weight', 'layers.2.input_layernorm.weight', 'layers.8.input_layernorm.weight', 'layers.13.input_layernorm.bias', 'layers.3.mlp.dense_4h_to_h.bias', 'layers.4.mlp.dense_h_to_4h.bias', 'layers.9.attention.query_key_value.bias', 'layers.5.mlp.dense_4h_to_h.bias', 'layers.0.input_layernorm.bias', 'layers.14.mlp.dense_4h_to_h.bias', 'layers.11.mlp.dense_h_to_4h.bias', 'layers.6.attention.query_key_value.bias', 'layers.9.mlp.dense_4h_to_h.bias', 'layers.2.post_attention_layernorm.weight', 'layers.9.mlp.dense_4h_to_h.weight', 'layers.11.input_layernorm.weight', 'layers.8.mlp.dense_4h_to_h.bias', 'layers.2.attention.query_key_value.bias', 'layers.5.post_attention_layernorm.weight', 'layers.5.attention.dense.weight', 'layers.10.mlp.dense_4h_to_h.weight', 'layers.14.post_attention_layernorm.bias', 'layers.13.attention.query_key_value.weight', 'layers.3.attention.query_key_value.bias', 'layers.10.input_layernorm.weight', 'layers.11.input_layernorm.bias', 'layers.14.mlp.dense_h_to_4h.weight', 'layers.13.mlp.dense_4h_to_h.bias', 'layers.11.post_attention_layernorm.bias', 'layers.4.attention.query_key_value.weight', 'layers.8.attention.dense.weight', 'layers.6.input_layernorm.weight', 'layers.5.attention.dense.bias', 'layers.6.post_attention_layernorm.weight', 'layers.4.input_layernorm.bias', 'layers.15.mlp.dense_h_to_4h.bias', 'final_layer_norm.weight', 'layers.3.mlp.dense_h_to_4h.bias', 'layers.13.post_attention_layernorm.bias', 'layers.8.attention.query_key_value.bias', 'layers.13.mlp.dense_4h_to_h.weight', 'layers.0.mlp.dense_4h_to_h.weight', 'layers.0.attention.query_key_value.bias', 'layers.13.mlp.dense_h_to_4h.weight', 'layers.15.attention.dense.bias', 'layers.1.input_layernorm.bias', 'layers.4.attention.query_key_value.bias', 'layers.12.mlp.dense_4h_to_h.bias', 'layers.9.attention.query_key_value.weight', 'layers.15.mlp.dense_4h_to_h.bias', 'layers.12.attention.dense.weight', 'layers.0.mlp.dense_4h_to_h.bias', 'layers.1.attention.query_key_value.bias', 'layers.2.mlp.dense_h_to_4h.weight', 'layers.7.attention.dense.bias', 'layers.14.attention.dense.weight', 'layers.1.mlp.dense_4h_to_h.bias', 'layers.12.mlp.dense_h_to_4h.weight', 'layers.3.input_layernorm.bias', 'layers.14.mlp.dense_h_to_4h.bias', 'layers.2.attention.dense.weight', 'layers.2.mlp.dense_4h_to_h.weight', 'layers.10.input_layernorm.bias', 'layers.5.attention.query_key_value.bias', 'layers.15.attention.query_key_value.bias', 'layers.4.attention.dense.bias', 'layers.7.post_attention_layernorm.weight', 'layers.12.attention.dense.bias', 'layers.12.attention.query_key_value.weight', 'layers.14.attention.dense.bias', 'layers.9.input_layernorm.weight', 'layers.1.post_attention_layernorm.weight', 'layers.8.post_attention_layernorm.bias', 'layers.13.attention.dense.bias', 'layers.12.attention.query_key_value.bias', 'layers.5.input_layernorm.bias', 'layers.11.attention.query_key_value.bias', 'layers.1.post_attention_layernorm.bias', 'layers.5.mlp.dense_4h_to_h.weight', 'layers.15.mlp.dense_h_to_4h.weight', 'layers.7.input_layernorm.bias', 'layers.14.attention.query_key_value.weight', 'layers.4.post_attention_layernorm.weight', 'layers.12.input_layernorm.weight', 'layers.6.mlp.dense_4h_to_h.weight', 'layers.9.attention.dense.weight', 'layers.5.post_attention_layernorm.bias', 'layers.13.attention.dense.weight', 'layers.0.mlp.dense_h_to_4h.weight', 'layers.7.mlp.dense_4h_to_h.weight', 'layers.2.attention.query_key_value.weight', 'layers.4.mlp.dense_4h_to_h.weight', 'layers.15.post_attention_layernorm.weight', 'layers.8.mlp.dense_h_to_4h.weight', 'layers.8.mlp.dense_4h_to_h.weight', 'layers.6.attention.dense.bias', 'layers.15.input_layernorm.bias', 'layers.5.attention.query_key_value.weight', 'layers.10.post_attention_layernorm.bias', 'layers.13.input_layernorm.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.mlp.dense_h_to_4h.bias', 'layers.4.input_layernorm.weight', 'layers.7.attention.query_key_value.bias', 'layers.7.mlp.dense_h_to_4h.bias', 'layers.15.attention.query_key_value.weight', 'layers.8.attention.query_key_value.weight', 'layers.10.attention.query_key_value.weight', 'layers.10.attention.dense.bias', 'layers.11.mlp.dense_4h_to_h.weight', 'layers.3.post_attention_layernorm.weight', 'layers.12.post_attention_layernorm.bias', 'layers.1.mlp.dense_h_to_4h.weight', 'layers.3.attention.dense.bias', 'layers.5.mlp.dense_h_to_4h.bias', 'layers.6.post_attention_layernorm.bias', 'layers.11.attention.dense.weight', 'layers.3.mlp.dense_h_to_4h.weight', 'layers.14.input_layernorm.weight', 'layers.2.mlp.dense_h_to_4h.bias', 'layers.4.mlp.dense_h_to_4h.weight', 'layers.6.attention.dense.weight', 'layers.7.attention.dense.weight', 'layers.12.post_attention_layernorm.weight', 'layers.10.mlp.dense_h_to_4h.weight', 'layers.3.attention.dense.weight', 'layers.9.mlp.dense_h_to_4h.weight', 'layers.8.mlp.dense_h_to_4h.bias', 'layers.10.post_attention_layernorm.weight', 'layers.0.attention.query_key_value.weight', 'layers.1.mlp.dense_4h_to_h.weight', 'layers.5.mlp.dense_h_to_4h.weight', 'layers.3.attention.query_key_value.weight', 'layers.4.mlp.dense_4h_to_h.bias', 'layers.14.attention.query_key_value.bias', 'layers.8.attention.dense.bias', 'layers.13.attention.query_key_value.bias', 'layers.12.mlp.dense_4h_to_h.weight', 'layers.7.post_attention_layernorm.bias', 'layers.11.mlp.dense_4h_to_h.bias', 'layers.12.input_layernorm.bias', 'layers.6.mlp.dense_h_to_4h.weight', 'layers.2.attention.dense.bias', 'layers.1.input_layernorm.weight', 'layers.3.input_layernorm.weight', 'layers.4.attention.dense.weight', 'layers.9.post_attention_layernorm.weight', 'layers.11.post_attention_layernorm.weight', 'layers.2.mlp.dense_4h_to_h.bias', 'layers.5.input_layernorm.weight', 'layers.13.mlp.dense_h_to_4h.bias', 'layers.9.mlp.dense_h_to_4h.bias', 'layers.10.mlp.dense_h_to_4h.bias', 'layers.1.attention.dense.weight', 'layers.14.post_attention_layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs were not used when initializing GPTNeoXForCausalLM: ['base_model.gpt_neox.layers.7.attention.query_key_value.weight', 'base_model.gpt_neox.layers.0.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.11.input_layernorm.bias', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.11.attention.query_key_value.bias', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.4.attention.query_key_value.bias', 'base_model.gpt_neox.layers.11.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.1.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.1.input_layernorm.bias', 'base_model.gpt_neox.layers.10.attention.dense.bias', 'frozen_head.decoder_blocks.0.input_layernorm.bias', 'frozen_head.decoder_blocks.0.attention.query_key_value.bias', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.3.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.6.input_layernorm.bias', 'base_model.gpt_neox.layers.12.attention.query_key_value.weight', 'base_model.gpt_neox.layers.13.attention.dense.bias', 'base_model.gpt_neox.layers.15.attention.dense.bias', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.3.attention.dense.bias', 'frozen_head.decoder_blocks.3.attention.dense.weight', 'base_model.gpt_neox.layers.1.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.14.attention.query_key_value.weight', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.attention.query_key_value.bias', 'frozen_head.decoder_blocks.3.attention.query_key_value.bias', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.5.input_layernorm.bias', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.1.input_layernorm.weight', 'frozen_head.decoder_blocks.2.attention.dense.bias', 'base_model.gpt_neox.layers.1.attention.dense.bias', 'base_model.gpt_neox.layers.15.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.1.attention.dense.bias', 'base_model.gpt_neox.final_layer_norm.bias', 'base_model.gpt_neox.layers.11.attention.query_key_value.weight', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.5.attention.dense.weight', 'base_model.gpt_neox.layers.8.input_layernorm.weight', 'frozen_head.decoder_blocks.3.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.2.attention.query_key_value.weight', 'base_model.gpt_neox.layers.4.input_layernorm.weight', 'frozen_head.final_norm.bias', 'base_model.gpt_neox.layers.1.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.10.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.1.attention.query_key_value.bias', 'base_model.gpt_neox.layers.5.attention.query_key_value.weight', 'base_model.gpt_neox.layers.6.input_layernorm.weight', 'base_model.embed_out.weight', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.10.attention.query_key_value.weight', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.5.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.4.input_layernorm.bias', 'base_model.gpt_neox.layers.13.input_layernorm.bias', 'base_model.gpt_neox.layers.5.input_layernorm.weight', 'base_model.gpt_neox.layers.7.attention.dense.weight', 'frozen_head.decoder_blocks.0.attention.dense.bias', 'frozen_head.decoder_blocks.3.input_layernorm.bias', 'base_model.gpt_neox.layers.9.attention.query_key_value.bias', 'v_head.2.weight', 'frozen_head.lm_head.weight', 'base_model.gpt_neox.layers.13.attention.dense.weight', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.8.attention.dense.weight', 'frozen_head.decoder_blocks.1.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.2.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.attention.query_key_value.weight', 'base_model.gpt_neox.layers.1.input_layernorm.bias', 'frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.10.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.15.input_layernorm.weight', 'base_model.gpt_neox.layers.14.attention.dense.bias', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.2.attention.dense.weight', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.2.attention.dense.weight', 'base_model.gpt_neox.layers.13.attention.query_key_value.bias', 'base_model.gpt_neox.layers.7.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.2.attention.query_key_value.weight', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.attention.dense.weight', 'base_model.gpt_neox.layers.5.attention.dense.bias', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.5.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.8.attention.query_key_value.weight', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.0.attention.query_key_value.weight', 'base_model.gpt_neox.layers.4.attention.dense.weight', 'base_model.gpt_neox.layers.9.attention.dense.bias', 'frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.0.attention.query_key_value.weight', 'base_model.gpt_neox.layers.1.attention.query_key_value.weight', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.2.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.1.attention.dense.weight', 'frozen_head.decoder_blocks.3.attention.query_key_value.weight', 'base_model.gpt_neox.layers.0.attention.query_key_value.bias', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.14.input_layernorm.weight', 'base_model.gpt_neox.layers.0.input_layernorm.weight', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.15.input_layernorm.bias', 'frozen_head.decoder_blocks.1.attention.dense.weight', 'base_model.gpt_neox.layers.13.input_layernorm.weight', 'base_model.gpt_neox.layers.13.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.5.attention.query_key_value.bias', 'frozen_head.decoder_blocks.2.input_layernorm.weight', 'base_model.gpt_neox.layers.12.input_layernorm.bias', 'frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.12.attention.dense.weight', 'base_model.gpt_neox.layers.8.input_layernorm.bias', 'base_model.gpt_neox.layers.7.input_layernorm.weight', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.14.attention.query_key_value.bias', 'base_model.gpt_neox.layers.0.input_layernorm.bias', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.11.input_layernorm.weight', 'base_model.gpt_neox.layers.15.attention.query_key_value.weight', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.weight', 'v_head.0.bias', 'base_model.gpt_neox.layers.0.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.4.attention.dense.bias', 'base_model.gpt_neox.layers.3.input_layernorm.bias', 'base_model.gpt_neox.layers.8.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.0.input_layernorm.weight', 'frozen_head.decoder_blocks.3.input_layernorm.weight', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.12.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.4.attention.query_key_value.weight', 'base_model.gpt_neox.layers.4.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.8.attention.dense.bias', 'base_model.gpt_neox.layers.7.attention.query_key_value.bias', 'base_model.gpt_neox.layers.6.attention.dense.weight', 'base_model.gpt_neox.embed_in.weight', 'base_model.gpt_neox.layers.2.attention.query_key_value.bias', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.6.attention.dense.bias', 'base_model.gpt_neox.layers.8.attention.query_key_value.bias', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.15.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.0.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.12.attention.dense.bias', 'base_model.gpt_neox.layers.15.attention.dense.weight', 'base_model.gpt_neox.layers.0.attention.dense.bias', 'base_model.gpt_neox.layers.12.input_layernorm.weight', 'base_model.gpt_neox.layers.6.attention.query_key_value.bias', 'base_model.gpt_neox.layers.3.attention.dense.bias', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.10.input_layernorm.bias', 'base_model.gpt_neox.layers.10.input_layernorm.weight', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.2.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.11.attention.dense.bias', 'frozen_head.decoder_blocks.0.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.14.attention.dense.weight', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.1.attention.query_key_value.weight', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.weight', 'v_head.0.weight', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.14.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.14.input_layernorm.bias', 'base_model.gpt_neox.layers.2.input_layernorm.bias', 'base_model.gpt_neox.layers.6.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.11.attention.dense.weight', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.13.attention.query_key_value.weight', 'base_model.gpt_neox.layers.15.attention.query_key_value.bias', 'base_model.gpt_neox.layers.7.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.14.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.3.input_layernorm.weight', 'base_model.gpt_neox.layers.6.attention.query_key_value.weight', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.2.attention.dense.bias', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.1.input_layernorm.weight', 'base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.10.attention.query_key_value.bias', 'base_model.gpt_neox.layers.2.input_layernorm.weight', 'base_model.gpt_neox.layers.7.input_layernorm.bias', 'base_model.gpt_neox.layers.8.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.9.attention.dense.weight', 'v_head.2.bias', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.final_layer_norm.weight', 'base_model.gpt_neox.layers.13.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.2.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.0.attention.dense.weight', 'base_model.gpt_neox.layers.6.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.9.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.12.attention.query_key_value.bias', 'base_model.gpt_neox.layers.9.post_attention_layernorm.weight', 'frozen_head.final_norm.weight', 'base_model.gpt_neox.layers.10.attention.dense.weight', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.11.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.9.input_layernorm.bias', 'base_model.gpt_neox.layers.0.attention.dense.weight', 'base_model.gpt_neox.layers.7.attention.dense.bias', 'base_model.gpt_neox.layers.9.attention.query_key_value.weight', 'base_model.gpt_neox.layers.12.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.1.attention.query_key_value.bias', 'base_model.gpt_neox.layers.4.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.2.input_layernorm.bias', 'base_model.gpt_neox.layers.9.input_layernorm.weight', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.2.attention.query_key_value.bias']
- This IS expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs and are newly initialized: ['layers.6.input_layernorm.weight', 'layers.4.mlp.dense_4h_to_h.bias', 'layers.10.post_attention_layernorm.bias', 'layers.9.input_layernorm.weight', 'layers.5.mlp.dense_4h_to_h.weight', 'layers.14.attention.dense.weight', 'layers.13.mlp.dense_h_to_4h.bias', 'layers.12.mlp.dense_h_to_4h.bias', 'layers.8.attention.query_key_value.bias', 'layers.7.input_layernorm.weight', 'layers.11.attention.query_key_value.bias', 'layers.6.attention.dense.bias', 'layers.13.mlp.dense_4h_to_h.bias', 'layers.5.attention.query_key_value.bias', 'layers.0.input_layernorm.weight', 'layers.7.mlp.dense_4h_to_h.bias', 'layers.14.mlp.dense_h_to_4h.bias', 'layers.10.mlp.dense_4h_to_h.weight', 'layers.4.mlp.dense_h_to_4h.bias', 'layers.5.post_attention_layernorm.bias', 'layers.12.attention.dense.weight', 'layers.7.mlp.dense_h_to_4h.bias', 'layers.15.post_attention_layernorm.weight', 'layers.2.input_layernorm.weight', 'layers.7.attention.dense.bias', 'layers.15.input_layernorm.weight', 'layers.4.attention.query_key_value.weight', 'layers.1.mlp.dense_4h_to_h.bias', 'layers.12.input_layernorm.weight', 'layers.1.input_layernorm.bias', 'layers.10.input_layernorm.weight', 'layers.1.attention.dense.bias', 'layers.9.post_attention_layernorm.bias', 'layers.9.mlp.dense_h_to_4h.bias', 'layers.7.mlp.dense_h_to_4h.weight', 'layers.3.mlp.dense_h_to_4h.weight', 'layers.0.attention.dense.bias', 'layers.13.attention.query_key_value.bias', 'layers.10.attention.dense.bias', 'final_layer_norm.bias', 'layers.4.attention.dense.weight', 'layers.1.post_attention_layernorm.weight', 'layers.7.post_attention_layernorm.weight', 'layers.11.post_attention_layernorm.bias', 'layers.4.attention.dense.bias', 'layers.6.attention.query_key_value.bias', 'layers.14.mlp.dense_4h_to_h.bias', 'layers.5.post_attention_layernorm.weight', 'layers.12.attention.dense.bias', 'layers.11.mlp.dense_h_to_4h.weight', 'layers.6.input_layernorm.bias', 'layers.0.mlp.dense_h_to_4h.bias', 'layers.13.mlp.dense_h_to_4h.weight', 'layers.3.attention.query_key_value.weight', 'layers.5.mlp.dense_4h_to_h.bias', 'layers.13.attention.dense.weight', 'layers.11.input_layernorm.weight', 'layers.13.input_layernorm.bias', 'layers.1.mlp.dense_h_to_4h.weight', 'layers.0.attention.query_key_value.bias', 'layers.5.attention.dense.bias', 'embed_out.weight', 'layers.2.post_attention_layernorm.weight', 'layers.15.attention.query_key_value.bias', 'layers.8.post_attention_layernorm.bias', 'layers.14.attention.query_key_value.bias', 'layers.7.input_layernorm.bias', 'layers.6.mlp.dense_h_to_4h.bias', 'layers.4.input_layernorm.bias', 'layers.12.post_attention_layernorm.bias', 'layers.2.mlp.dense_h_to_4h.bias', 'final_layer_norm.weight', 'layers.15.input_layernorm.bias', 'layers.8.post_attention_layernorm.weight', 'layers.12.attention.query_key_value.weight', 'layers.8.mlp.dense_h_to_4h.weight', 'layers.2.attention.query_key_value.weight', 'layers.15.attention.dense.weight', 'layers.1.post_attention_layernorm.bias', 'layers.8.attention.dense.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.attention.dense.weight', 'layers.3.mlp.dense_4h_to_h.bias', 'layers.11.attention.query_key_value.weight', 'layers.14.mlp.dense_4h_to_h.weight', 'layers.1.mlp.dense_4h_to_h.weight', 'layers.13.input_layernorm.weight', 'layers.9.attention.dense.weight', 'embed_in.weight', 'layers.3.mlp.dense_h_to_4h.bias', 'layers.5.attention.dense.weight', 'layers.8.attention.query_key_value.weight', 'layers.14.input_layernorm.weight', 'layers.6.mlp.dense_h_to_4h.weight', 'layers.4.mlp.dense_4h_to_h.weight', 'layers.1.attention.query_key_value.weight', 'layers.3.attention.dense.weight', 'layers.0.input_layernorm.bias', 'layers.2.mlp.dense_4h_to_h.bias', 'layers.7.attention.dense.weight', 'layers.0.mlp.dense_4h_to_h.weight', 'layers.14.post_attention_layernorm.bias', 'layers.12.input_layernorm.bias', 'layers.6.mlp.dense_4h_to_h.weight', 'layers.9.post_attention_layernorm.weight', 'layers.7.mlp.dense_4h_to_h.weight', 'layers.9.mlp.dense_4h_to_h.weight', 'layers.3.post_attention_layernorm.weight', 'layers.15.mlp.dense_h_to_4h.bias', 'layers.2.attention.query_key_value.bias', 'layers.6.attention.query_key_value.weight', 'layers.6.post_attention_layernorm.bias', 'layers.8.mlp.dense_4h_to_h.weight', 'layers.7.attention.query_key_value.weight', 'layers.10.input_layernorm.bias', 'layers.11.mlp.dense_4h_to_h.weight', 'layers.15.mlp.dense_h_to_4h.weight', 'layers.5.mlp.dense_h_to_4h.bias', 'layers.5.mlp.dense_h_to_4h.weight', 'layers.9.input_layernorm.bias', 'layers.14.attention.dense.bias', 'layers.0.attention.query_key_value.weight', 'layers.9.mlp.dense_4h_to_h.bias', 'layers.13.attention.dense.bias', 'layers.1.input_layernorm.weight', 'layers.14.input_layernorm.bias', 'layers.4.attention.query_key_value.bias', 'layers.3.mlp.dense_4h_to_h.weight', 'layers.9.attention.query_key_value.weight', 'layers.12.mlp.dense_h_to_4h.weight', 'layers.6.mlp.dense_4h_to_h.bias', 'layers.3.input_layernorm.weight', 'layers.11.attention.dense.bias', 'layers.12.mlp.dense_4h_to_h.bias', 'layers.4.input_layernorm.weight', 'layers.13.attention.query_key_value.weight', 'layers.3.attention.dense.bias', 'layers.1.attention.dense.weight', 'layers.9.attention.query_key_value.bias', 'layers.6.post_attention_layernorm.weight', 'layers.10.attention.query_key_value.bias', 'layers.1.attention.query_key_value.bias', 'layers.10.mlp.dense_4h_to_h.bias', 'layers.4.post_attention_layernorm.weight', 'layers.2.mlp.dense_4h_to_h.weight', 'layers.15.attention.query_key_value.weight', 'layers.11.mlp.dense_h_to_4h.bias', 'layers.8.mlp.dense_4h_to_h.bias', 'layers.8.input_layernorm.weight', 'layers.5.input_layernorm.bias', 'layers.12.mlp.dense_4h_to_h.weight', 'layers.4.post_attention_layernorm.bias', 'layers.12.attention.query_key_value.bias', 'layers.5.attention.query_key_value.weight', 'layers.3.post_attention_layernorm.bias', 'layers.11.input_layernorm.bias', 'layers.14.mlp.dense_h_to_4h.weight', 'layers.0.post_attention_layernorm.bias', 'layers.10.mlp.dense_h_to_4h.bias', 'layers.5.input_layernorm.weight', 'layers.10.post_attention_layernorm.weight', 'layers.0.mlp.dense_4h_to_h.bias', 'layers.15.mlp.dense_4h_to_h.bias', 'layers.8.mlp.dense_h_to_4h.bias', 'layers.10.attention.dense.weight', 'layers.0.post_attention_layernorm.weight', 'layers.8.attention.dense.bias', 'layers.0.attention.dense.weight', 'layers.7.attention.query_key_value.bias', 'layers.15.mlp.dense_4h_to_h.weight', 'layers.15.post_attention_layernorm.bias', 'layers.15.attention.dense.bias', 'layers.1.mlp.dense_h_to_4h.bias', 'layers.8.input_layernorm.bias', 'layers.2.mlp.dense_h_to_4h.weight', 'layers.11.mlp.dense_4h_to_h.bias', 'layers.2.attention.dense.weight', 'layers.10.attention.query_key_value.weight', 'layers.7.post_attention_layernorm.bias', 'layers.2.post_attention_layernorm.bias', 'layers.10.mlp.dense_h_to_4h.weight', 'layers.3.input_layernorm.bias', 'layers.14.post_attention_layernorm.weight', 'layers.9.mlp.dense_h_to_4h.weight', 'layers.12.post_attention_layernorm.weight', 'layers.4.mlp.dense_h_to_4h.weight', 'layers.9.attention.dense.bias', 'layers.3.attention.query_key_value.bias', 'layers.2.input_layernorm.bias', 'layers.6.attention.dense.weight', 'layers.2.attention.dense.bias', 'layers.13.mlp.dense_4h_to_h.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.post_attention_layernorm.bias', 'layers.14.attention.query_key_value.weight', 'layers.0.mlp.dense_h_to_4h.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs were not used when initializing GPTNeoXForCausalLM: ['base_model.gpt_neox.layers.0.attention.dense.weight', 'v_head.0.bias', 'base_model.gpt_neox.layers.11.input_layernorm.weight', 'base_model.gpt_neox.layers.1.input_layernorm.bias', 'base_model.gpt_neox.layers.4.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.5.input_layernorm.bias', 'base_model.gpt_neox.layers.7.input_layernorm.weight', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.weight', 'frozen_head.lm_head.weight', 'base_model.gpt_neox.layers.1.input_layernorm.weight', 'base_model.gpt_neox.layers.13.input_layernorm.weight', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.15.attention.query_key_value.bias', 'frozen_head.decoder_blocks.2.attention.query_key_value.bias', 'base_model.gpt_neox.layers.8.input_layernorm.weight', 'base_model.gpt_neox.layers.1.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.15.attention.dense.weight', 'base_model.gpt_neox.layers.5.attention.query_key_value.weight', 'base_model.gpt_neox.layers.8.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.9.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.final_layer_norm.weight', 'frozen_head.decoder_blocks.1.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.0.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.11.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.15.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.0.attention.dense.bias', 'base_model.gpt_neox.layers.9.attention.dense.weight', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.13.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.1.attention.dense.bias', 'base_model.gpt_neox.layers.6.attention.dense.bias', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.0.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.4.attention.query_key_value.weight', 'frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.2.attention.dense.weight', 'base_model.gpt_neox.layers.2.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.13.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.15.input_layernorm.bias', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.15.attention.query_key_value.weight', 'base_model.gpt_neox.layers.6.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.3.attention.dense.bias', 'frozen_head.decoder_blocks.3.attention.query_key_value.weight', 'base_model.gpt_neox.layers.0.input_layernorm.weight', 'frozen_head.decoder_blocks.3.attention.query_key_value.bias', 'frozen_head.decoder_blocks.1.attention.dense.weight', 'base_model.gpt_neox.layers.13.attention.dense.weight', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.0.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.3.attention.query_key_value.weight', 'base_model.gpt_neox.layers.1.attention.query_key_value.bias', 'base_model.gpt_neox.layers.6.attention.dense.weight', 'frozen_head.decoder_blocks.1.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.15.attention.dense.bias', 'base_model.gpt_neox.layers.5.attention.dense.weight', 'frozen_head.decoder_blocks.1.attention.query_key_value.bias', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.8.attention.query_key_value.weight', 'base_model.gpt_neox.layers.10.input_layernorm.weight', 'frozen_head.final_norm.bias', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.4.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.4.input_layernorm.bias', 'base_model.gpt_neox.layers.6.attention.query_key_value.bias', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.7.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.6.post_attention_layernorm.bias', 'base_model.embed_out.weight', 'base_model.gpt_neox.layers.3.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.10.attention.query_key_value.bias', 'base_model.gpt_neox.layers.7.attention.dense.weight', 'frozen_head.decoder_blocks.0.attention.dense.weight', 'base_model.gpt_neox.layers.14.attention.query_key_value.weight', 'base_model.gpt_neox.layers.0.attention.query_key_value.bias', 'base_model.gpt_neox.layers.5.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.8.attention.query_key_value.bias', 'base_model.gpt_neox.layers.0.attention.query_key_value.weight', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.3.input_layernorm.weight', 'base_model.gpt_neox.layers.11.attention.dense.weight', 'base_model.gpt_neox.layers.12.input_layernorm.weight', 'base_model.gpt_neox.layers.2.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.0.attention.dense.bias', 'base_model.gpt_neox.layers.14.input_layernorm.bias', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.8.attention.dense.bias', 'v_head.2.bias', 'base_model.gpt_neox.layers.11.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.9.attention.dense.bias', 'base_model.gpt_neox.layers.3.attention.query_key_value.bias', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.5.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.6.input_layernorm.weight', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.11.attention.query_key_value.bias', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.bias', 'frozen_head.final_norm.weight', 'base_model.gpt_neox.layers.15.input_layernorm.weight', 'base_model.gpt_neox.layers.9.attention.query_key_value.bias', 'base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.3.attention.dense.weight', 'frozen_head.decoder_blocks.0.attention.query_key_value.bias', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.2.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.3.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.0.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.2.attention.dense.weight', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.2.attention.query_key_value.weight', 'base_model.gpt_neox.layers.7.input_layernorm.bias', 'base_model.gpt_neox.layers.11.attention.query_key_value.weight', 'frozen_head.decoder_blocks.1.input_layernorm.bias', 'base_model.gpt_neox.layers.2.attention.dense.bias', 'base_model.gpt_neox.layers.11.input_layernorm.bias', 'frozen_head.decoder_blocks.1.input_layernorm.weight', 'v_head.0.weight', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.5.attention.query_key_value.bias', 'base_model.gpt_neox.layers.10.attention.query_key_value.weight', 'base_model.gpt_neox.layers.1.attention.dense.weight', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.10.attention.dense.bias', 'base_model.gpt_neox.layers.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.14.attention.query_key_value.bias', 'base_model.gpt_neox.layers.4.input_layernorm.weight', 'frozen_head.decoder_blocks.2.attention.dense.bias', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.final_layer_norm.bias', 'frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.11.attention.dense.bias', 'base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.weight', 'v_head.2.weight', 'base_model.gpt_neox.layers.13.input_layernorm.bias', 'base_model.gpt_neox.layers.12.input_layernorm.bias', 'base_model.gpt_neox.layers.8.attention.dense.weight', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.12.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.input_layernorm.weight', 'base_model.gpt_neox.layers.4.attention.dense.weight', 'base_model.gpt_neox.layers.6.attention.query_key_value.weight', 'base_model.gpt_neox.layers.1.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.15.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.2.input_layernorm.bias', 'base_model.gpt_neox.layers.7.attention.dense.bias', 'base_model.gpt_neox.layers.7.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.9.input_layernorm.bias', 'base_model.gpt_neox.layers.7.attention.query_key_value.weight', 'base_model.gpt_neox.layers.1.attention.dense.bias', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.2.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.12.attention.query_key_value.weight', 'base_model.gpt_neox.layers.9.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.0.input_layernorm.bias', 'frozen_head.decoder_blocks.3.input_layernorm.bias', 'base_model.gpt_neox.layers.4.attention.query_key_value.bias', 'frozen_head.decoder_blocks.2.input_layernorm.weight', 'base_model.gpt_neox.layers.9.attention.query_key_value.weight', 'base_model.gpt_neox.layers.3.attention.dense.weight', 'base_model.gpt_neox.layers.14.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.8.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.0.attention.query_key_value.weight', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.13.attention.query_key_value.bias', 'base_model.gpt_neox.layers.10.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.0.input_layernorm.bias', 'base_model.gpt_neox.layers.12.attention.dense.weight', 'base_model.gpt_neox.layers.4.attention.dense.bias', 'frozen_head.decoder_blocks.0.input_layernorm.weight', 'base_model.gpt_neox.layers.3.input_layernorm.bias', 'base_model.gpt_neox.layers.13.attention.dense.bias', 'base_model.gpt_neox.layers.10.attention.dense.weight', 'base_model.gpt_neox.embed_in.weight', 'frozen_head.decoder_blocks.3.attention.dense.bias', 'base_model.gpt_neox.layers.1.attention.query_key_value.weight', 'base_model.gpt_neox.layers.14.attention.dense.bias', 'base_model.gpt_neox.layers.14.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.12.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.12.attention.dense.bias', 'base_model.gpt_neox.layers.7.attention.query_key_value.bias', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.2.input_layernorm.bias', 'base_model.gpt_neox.layers.14.input_layernorm.weight', 'base_model.gpt_neox.layers.10.input_layernorm.bias', 'base_model.gpt_neox.layers.2.attention.query_key_value.bias', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.6.input_layernorm.bias', 'base_model.gpt_neox.layers.2.input_layernorm.weight', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.2.attention.query_key_value.weight', 'base_model.gpt_neox.layers.5.input_layernorm.weight', 'frozen_head.decoder_blocks.1.attention.query_key_value.weight', 'base_model.gpt_neox.layers.13.attention.query_key_value.weight', 'base_model.gpt_neox.layers.12.attention.query_key_value.bias', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.9.input_layernorm.weight', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.14.attention.dense.weight', 'base_model.gpt_neox.layers.10.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.8.input_layernorm.bias', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.5.attention.dense.bias', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.bias']
- This IS expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs and are newly initialized: ['layers.11.input_layernorm.weight', 'layers.1.attention.query_key_value.weight', 'layers.1.post_attention_layernorm.bias', 'layers.13.attention.query_key_value.weight', 'layers.9.mlp.dense_4h_to_h.bias', 'layers.12.mlp.dense_h_to_4h.bias', 'layers.15.mlp.dense_4h_to_h.bias', 'layers.15.post_attention_layernorm.weight', 'layers.0.post_attention_layernorm.bias', 'layers.1.input_layernorm.bias', 'layers.15.attention.query_key_value.weight', 'layers.1.attention.dense.bias', 'layers.7.post_attention_layernorm.weight', 'layers.13.post_attention_layernorm.weight', 'layers.4.input_layernorm.bias', 'layers.15.mlp.dense_4h_to_h.weight', 'layers.7.mlp.dense_4h_to_h.bias', 'layers.13.input_layernorm.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.input_layernorm.bias', 'layers.2.post_attention_layernorm.weight', 'layers.6.attention.dense.bias', 'layers.11.post_attention_layernorm.weight', 'layers.7.attention.query_key_value.bias', 'layers.11.mlp.dense_4h_to_h.weight', 'layers.7.attention.dense.weight', 'layers.14.mlp.dense_4h_to_h.bias', 'layers.12.input_layernorm.bias', 'layers.12.input_layernorm.weight', 'layers.3.attention.dense.weight', 'layers.11.attention.dense.weight', 'layers.6.mlp.dense_h_to_4h.weight', 'layers.14.post_attention_layernorm.weight', 'layers.2.attention.query_key_value.bias', 'layers.8.attention.query_key_value.weight', 'layers.15.attention.dense.bias', 'layers.8.mlp.dense_4h_to_h.weight', 'layers.8.input_layernorm.weight', 'layers.2.attention.dense.weight', 'layers.4.post_attention_layernorm.weight', 'layers.5.mlp.dense_h_to_4h.weight', 'layers.11.mlp.dense_4h_to_h.bias', 'layers.3.attention.dense.bias', 'layers.4.input_layernorm.weight', 'layers.1.mlp.dense_4h_to_h.weight', 'layers.7.input_layernorm.bias', 'layers.1.mlp.dense_h_to_4h.bias', 'layers.11.attention.query_key_value.weight', 'layers.13.attention.dense.bias', 'embed_out.weight', 'layers.15.input_layernorm.bias', 'layers.13.mlp.dense_h_to_4h.weight', 'layers.11.attention.dense.bias', 'layers.2.input_layernorm.weight', 'layers.6.attention.query_key_value.weight', 'layers.9.attention.dense.bias', 'layers.4.attention.query_key_value.bias', 'layers.2.post_attention_layernorm.bias', 'layers.3.attention.query_key_value.weight', 'final_layer_norm.bias', 'layers.7.post_attention_layernorm.bias', 'layers.7.mlp.dense_4h_to_h.weight', 'layers.10.mlp.dense_4h_to_h.bias', 'layers.7.mlp.dense_h_to_4h.bias', 'layers.6.mlp.dense_4h_to_h.weight', 'layers.14.mlp.dense_h_to_4h.bias', 'layers.5.input_layernorm.weight', 'layers.14.attention.query_key_value.weight', 'layers.8.post_attention_layernorm.weight', 'layers.4.mlp.dense_4h_to_h.bias', 'layers.15.input_layernorm.weight', 'layers.13.post_attention_layernorm.bias', 'layers.7.mlp.dense_h_to_4h.weight', 'layers.2.mlp.dense_4h_to_h.weight', 'layers.6.attention.query_key_value.bias', 'layers.1.attention.dense.weight', 'layers.10.mlp.dense_4h_to_h.weight', 'layers.5.attention.query_key_value.bias', 'layers.4.mlp.dense_h_to_4h.bias', 'layers.8.mlp.dense_h_to_4h.weight', 'layers.14.input_layernorm.bias', 'layers.10.attention.dense.bias', 'layers.7.input_layernorm.weight', 'layers.7.attention.dense.bias', 'layers.9.mlp.dense_h_to_4h.weight', 'layers.3.post_attention_layernorm.weight', 'layers.5.post_attention_layernorm.bias', 'layers.10.input_layernorm.weight', 'layers.10.input_layernorm.bias', 'layers.11.mlp.dense_h_to_4h.bias', 'layers.12.post_attention_layernorm.weight', 'layers.1.input_layernorm.weight', 'layers.0.mlp.dense_h_to_4h.weight', 'layers.6.mlp.dense_h_to_4h.bias', 'layers.3.input_layernorm.bias', 'layers.6.attention.dense.weight', 'layers.0.mlp.dense_4h_to_h.bias', 'layers.6.post_attention_layernorm.bias', 'layers.10.mlp.dense_h_to_4h.bias', 'layers.6.input_layernorm.weight', 'layers.5.attention.dense.bias', 'layers.9.post_attention_layernorm.bias', 'layers.12.attention.dense.weight', 'layers.15.attention.query_key_value.bias', 'layers.2.mlp.dense_4h_to_h.bias', 'layers.13.attention.query_key_value.bias', 'layers.4.attention.dense.bias', 'layers.0.input_layernorm.weight', 'layers.0.input_layernorm.bias', 'layers.12.mlp.dense_4h_to_h.bias', 'embed_in.weight', 'layers.2.attention.dense.bias', 'layers.14.input_layernorm.weight', 'layers.8.mlp.dense_h_to_4h.bias', 'layers.9.mlp.dense_4h_to_h.weight', 'layers.14.mlp.dense_4h_to_h.weight', 'layers.15.mlp.dense_h_to_4h.weight', 'layers.10.attention.query_key_value.weight', 'layers.15.post_attention_layernorm.bias', 'layers.12.mlp.dense_4h_to_h.weight', 'layers.9.attention.query_key_value.weight', 'layers.8.mlp.dense_4h_to_h.bias', 'layers.8.attention.dense.bias', 'layers.10.post_attention_layernorm.weight', 'layers.10.attention.dense.weight', 'layers.9.post_attention_layernorm.weight', 'layers.3.mlp.dense_h_to_4h.bias', 'layers.8.attention.query_key_value.bias', 'layers.11.attention.query_key_value.bias', 'layers.0.attention.query_key_value.bias', 'layers.3.mlp.dense_4h_to_h.weight', 'layers.14.mlp.dense_h_to_4h.weight', 'layers.9.input_layernorm.weight', 'layers.13.input_layernorm.bias', 'layers.3.post_attention_layernorm.bias', 'layers.1.mlp.dense_4h_to_h.bias', 'layers.0.attention.dense.bias', 'layers.9.attention.query_key_value.bias', 'layers.14.post_attention_layernorm.bias', 'layers.1.post_attention_layernorm.weight', 'layers.2.mlp.dense_h_to_4h.weight', 'layers.4.mlp.dense_4h_to_h.weight', 'layers.9.mlp.dense_h_to_4h.bias', 'layers.11.input_layernorm.bias', 'layers.9.input_layernorm.bias', 'layers.14.attention.dense.bias', 'layers.10.post_attention_layernorm.bias', 'layers.6.post_attention_layernorm.weight', 'layers.3.attention.query_key_value.bias', 'layers.8.post_attention_layernorm.bias', 'layers.12.attention.dense.bias', 'layers.4.attention.query_key_value.weight', 'layers.0.attention.dense.weight', 'layers.0.attention.query_key_value.weight', 'layers.1.mlp.dense_h_to_4h.weight', 'layers.4.attention.dense.weight', 'layers.3.input_layernorm.weight', 'layers.11.mlp.dense_h_to_4h.weight', 'layers.12.attention.query_key_value.weight', 'layers.13.mlp.dense_4h_to_h.bias', 'layers.15.attention.dense.weight', 'layers.8.attention.dense.weight', 'layers.5.attention.query_key_value.weight', 'layers.12.mlp.dense_h_to_4h.weight', 'layers.5.mlp.dense_4h_to_h.bias', 'layers.0.mlp.dense_4h_to_h.weight', 'layers.0.mlp.dense_h_to_4h.bias', 'layers.5.mlp.dense_4h_to_h.weight', 'layers.13.attention.dense.weight', 'layers.4.mlp.dense_h_to_4h.weight', 'final_layer_norm.weight', 'layers.6.mlp.dense_4h_to_h.bias', 'layers.3.mlp.dense_4h_to_h.bias', 'layers.1.attention.query_key_value.bias', 'layers.3.mlp.dense_h_to_4h.weight', 'layers.5.mlp.dense_h_to_4h.bias', 'layers.2.attention.query_key_value.weight', 'layers.10.mlp.dense_h_to_4h.weight', 'layers.2.input_layernorm.bias', 'layers.10.attention.query_key_value.bias', 'layers.4.post_attention_layernorm.bias', 'layers.0.post_attention_layernorm.weight', 'layers.2.mlp.dense_h_to_4h.bias', 'layers.9.attention.dense.weight', 'layers.13.mlp.dense_h_to_4h.bias', 'layers.13.mlp.dense_4h_to_h.weight', 'layers.7.attention.query_key_value.weight', 'layers.14.attention.dense.weight', 'layers.6.input_layernorm.bias', 'layers.14.attention.query_key_value.bias', 'layers.5.attention.dense.weight', 'layers.8.input_layernorm.bias', 'layers.15.mlp.dense_h_to_4h.bias', 'layers.12.attention.query_key_value.bias', 'layers.12.post_attention_layernorm.bias', 'layers.11.post_attention_layernorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs were not used when initializing GPTNeoXForCausalLM: ['frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.2.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.12.attention.dense.bias', 'base_model.gpt_neox.layers.13.attention.dense.bias', 'base_model.gpt_neox.layers.12.attention.query_key_value.weight', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.9.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.0.attention.dense.bias', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.1.attention.dense.weight', 'frozen_head.decoder_blocks.1.attention.dense.bias', 'base_model.gpt_neox.layers.4.input_layernorm.bias', 'base_model.gpt_neox.layers.0.attention.query_key_value.bias', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.9.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.0.input_layernorm.bias', 'base_model.gpt_neox.layers.15.input_layernorm.weight', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.3.attention.query_key_value.weight', 'base_model.gpt_neox.layers.8.attention.query_key_value.weight', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.weight', 'base_model.embed_out.weight', 'base_model.gpt_neox.layers.0.attention.dense.bias', 'base_model.gpt_neox.layers.0.attention.dense.weight', 'base_model.gpt_neox.layers.6.input_layernorm.bias', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.12.attention.dense.weight', 'base_model.gpt_neox.layers.5.input_layernorm.weight', 'base_model.gpt_neox.layers.4.attention.query_key_value.weight', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.0.input_layernorm.weight', 'base_model.gpt_neox.layers.10.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.3.attention.dense.weight', 'base_model.gpt_neox.layers.13.attention.query_key_value.weight', 'base_model.gpt_neox.layers.1.attention.dense.weight', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.9.input_layernorm.weight', 'base_model.gpt_neox.layers.6.attention.query_key_value.bias', 'base_model.gpt_neox.layers.8.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.15.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.13.input_layernorm.weight', 'base_model.gpt_neox.layers.10.attention.dense.weight', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.13.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.0.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.2.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.4.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.9.input_layernorm.bias', 'base_model.gpt_neox.layers.5.attention.query_key_value.bias', 'base_model.gpt_neox.layers.7.attention.dense.bias', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.5.attention.dense.weight', 'base_model.gpt_neox.layers.1.attention.dense.bias', 'base_model.gpt_neox.layers.4.attention.query_key_value.bias', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.14.attention.dense.weight', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.0.post_attention_layernorm.weight', 'frozen_head.final_norm.bias', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.2.input_layernorm.bias', 'base_model.gpt_neox.layers.15.attention.query_key_value.weight', 'base_model.gpt_neox.layers.10.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.2.input_layernorm.weight', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.3.input_layernorm.weight', 'frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.15.input_layernorm.bias', 'base_model.gpt_neox.layers.5.attention.query_key_value.weight', 'base_model.gpt_neox.layers.3.attention.dense.bias', 'base_model.gpt_neox.layers.15.attention.dense.weight', 'frozen_head.decoder_blocks.2.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.9.attention.dense.weight', 'frozen_head.decoder_blocks.0.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.1.attention.query_key_value.weight', 'base_model.gpt_neox.layers.12.attention.query_key_value.bias', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.11.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.2.attention.query_key_value.bias', 'frozen_head.decoder_blocks.1.attention.query_key_value.weight', 'base_model.gpt_neox.layers.7.input_layernorm.bias', 'base_model.gpt_neox.layers.7.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.0.attention.query_key_value.weight', 'frozen_head.decoder_blocks.1.input_layernorm.bias', 'base_model.gpt_neox.layers.1.input_layernorm.bias', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.final_layer_norm.bias', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.4.attention.dense.bias', 'base_model.gpt_neox.layers.10.input_layernorm.weight', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.3.attention.query_key_value.bias', 'frozen_head.decoder_blocks.2.input_layernorm.bias', 'base_model.gpt_neox.layers.9.attention.query_key_value.weight', 'frozen_head.final_norm.weight', 'base_model.gpt_neox.layers.6.attention.dense.bias', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.attention.dense.weight', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.7.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.0.attention.query_key_value.bias', 'base_model.gpt_neox.layers.13.attention.query_key_value.bias', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.8.attention.query_key_value.bias', 'base_model.gpt_neox.layers.5.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.14.attention.dense.bias', 'base_model.gpt_neox.layers.8.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.2.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.11.attention.dense.bias', 'base_model.gpt_neox.layers.13.attention.dense.weight', 'base_model.gpt_neox.layers.11.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.8.attention.dense.weight', 'base_model.gpt_neox.layers.2.attention.query_key_value.weight', 'frozen_head.decoder_blocks.0.input_layernorm.weight', 'base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.5.input_layernorm.bias', 'base_model.gpt_neox.layers.4.input_layernorm.weight', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.3.input_layernorm.weight', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.11.input_layernorm.weight', 'base_model.gpt_neox.layers.14.input_layernorm.weight', 'base_model.gpt_neox.layers.5.attention.dense.bias', 'frozen_head.lm_head.weight', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.0.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.4.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.2.attention.dense.bias', 'base_model.gpt_neox.layers.3.attention.query_key_value.weight', 'base_model.gpt_neox.layers.11.attention.query_key_value.bias', 'base_model.gpt_neox.layers.11.attention.dense.weight', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.2.attention.dense.bias', 'frozen_head.decoder_blocks.2.attention.dense.weight', 'frozen_head.decoder_blocks.3.input_layernorm.bias', 'base_model.gpt_neox.layers.9.attention.query_key_value.bias', 'frozen_head.decoder_blocks.3.attention.query_key_value.bias', 'frozen_head.decoder_blocks.1.attention.query_key_value.bias', 'base_model.gpt_neox.layers.7.attention.query_key_value.bias', 'base_model.gpt_neox.layers.1.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.5.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.6.attention.dense.weight', 'base_model.gpt_neox.layers.3.input_layernorm.bias', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.13.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.6.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.2.attention.query_key_value.weight', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.0.input_layernorm.bias', 'base_model.gpt_neox.layers.2.attention.dense.weight', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.12.input_layernorm.bias', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.7.attention.dense.weight', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.14.input_layernorm.bias', 'base_model.gpt_neox.layers.15.attention.query_key_value.bias', 'base_model.gpt_neox.layers.11.input_layernorm.bias', 'base_model.gpt_neox.layers.10.input_layernorm.bias', 'frozen_head.decoder_blocks.0.attention.dense.weight', 'base_model.gpt_neox.layers.8.input_layernorm.weight', 'frozen_head.decoder_blocks.2.attention.query_key_value.bias', 'base_model.gpt_neox.layers.10.attention.dense.bias', 'base_model.gpt_neox.layers.8.attention.dense.bias', 'base_model.gpt_neox.layers.9.attention.dense.bias', 'base_model.gpt_neox.layers.10.attention.query_key_value.weight', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.6.input_layernorm.weight', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.bias', 'v_head.2.bias', 'base_model.gpt_neox.layers.7.attention.query_key_value.weight', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.1.input_layernorm.weight', 'base_model.gpt_neox.layers.12.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.1.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.12.input_layernorm.weight', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.bias', 'v_head.0.weight', 'base_model.gpt_neox.layers.11.attention.query_key_value.weight', 'base_model.gpt_neox.layers.14.attention.query_key_value.weight', 'base_model.gpt_neox.layers.7.input_layernorm.weight', 'v_head.0.bias', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.10.attention.query_key_value.bias', 'frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.14.attention.query_key_value.bias', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.13.input_layernorm.bias', 'base_model.gpt_neox.layers.15.attention.dense.bias', 'base_model.gpt_neox.layers.1.attention.query_key_value.bias', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.1.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.1.input_layernorm.weight', 'base_model.gpt_neox.layers.3.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.1.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.8.input_layernorm.bias', 'frozen_head.decoder_blocks.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.14.post_attention_layernorm.bias', 'v_head.2.weight', 'base_model.gpt_neox.layers.0.attention.query_key_value.weight', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.6.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.4.attention.dense.weight', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.3.attention.dense.bias', 'frozen_head.decoder_blocks.3.post_attention_layernorm.weight', 'base_model.gpt_neox.final_layer_norm.weight', 'base_model.gpt_neox.layers.6.attention.query_key_value.weight', 'base_model.gpt_neox.layers.15.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.embed_in.weight', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.2.input_layernorm.weight', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.14.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.12.post_attention_layernorm.weight']
- This IS expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs and are newly initialized: ['layers.15.attention.query_key_value.bias', 'layers.1.post_attention_layernorm.weight', 'layers.13.mlp.dense_4h_to_h.bias', 'layers.2.input_layernorm.weight', 'layers.1.post_attention_layernorm.bias', 'layers.1.attention.dense.weight', 'layers.3.mlp.dense_h_to_4h.weight', 'layers.5.post_attention_layernorm.weight', 'layers.10.attention.dense.weight', 'layers.15.attention.dense.bias', 'layers.3.mlp.dense_h_to_4h.bias', 'layers.11.input_layernorm.weight', 'layers.11.attention.dense.bias', 'layers.5.post_attention_layernorm.bias', 'layers.1.mlp.dense_h_to_4h.bias', 'layers.0.mlp.dense_4h_to_h.weight', 'layers.7.post_attention_layernorm.weight', 'layers.8.input_layernorm.weight', 'layers.11.post_attention_layernorm.weight', 'layers.10.input_layernorm.weight', 'layers.15.input_layernorm.bias', 'layers.3.attention.dense.weight', 'layers.10.attention.query_key_value.weight', 'layers.12.mlp.dense_h_to_4h.weight', 'layers.10.post_attention_layernorm.weight', 'layers.2.attention.dense.weight', 'layers.14.mlp.dense_4h_to_h.bias', 'layers.15.post_attention_layernorm.weight', 'embed_out.weight', 'layers.14.post_attention_layernorm.bias', 'layers.3.mlp.dense_4h_to_h.weight', 'layers.8.attention.dense.bias', 'layers.7.mlp.dense_4h_to_h.weight', 'layers.14.attention.dense.weight', 'layers.0.mlp.dense_h_to_4h.bias', 'layers.13.post_attention_layernorm.weight', 'layers.9.mlp.dense_h_to_4h.bias', 'layers.8.attention.query_key_value.weight', 'layers.13.mlp.dense_4h_to_h.weight', 'layers.2.attention.query_key_value.weight', 'layers.9.post_attention_layernorm.bias', 'layers.2.input_layernorm.bias', 'layers.15.attention.query_key_value.weight', 'layers.10.attention.dense.bias', 'layers.0.attention.dense.weight', 'layers.6.mlp.dense_4h_to_h.weight', 'layers.2.mlp.dense_h_to_4h.bias', 'layers.10.mlp.dense_h_to_4h.bias', 'layers.0.post_attention_layernorm.weight', 'layers.2.post_attention_layernorm.bias', 'layers.6.mlp.dense_h_to_4h.weight', 'layers.3.input_layernorm.weight', 'layers.7.mlp.dense_h_to_4h.bias', 'layers.4.attention.dense.bias', 'layers.2.attention.query_key_value.bias', 'layers.0.attention.dense.bias', 'layers.10.attention.query_key_value.bias', 'layers.0.attention.query_key_value.weight', 'layers.2.attention.dense.bias', 'layers.8.attention.query_key_value.bias', 'layers.14.mlp.dense_4h_to_h.weight', 'layers.4.mlp.dense_h_to_4h.weight', 'layers.12.attention.dense.bias', 'layers.12.mlp.dense_h_to_4h.bias', 'layers.0.input_layernorm.bias', 'layers.12.attention.query_key_value.weight', 'layers.14.attention.query_key_value.weight', 'layers.14.input_layernorm.bias', 'layers.14.post_attention_layernorm.weight', 'layers.12.input_layernorm.bias', 'layers.11.mlp.dense_h_to_4h.weight', 'layers.9.attention.dense.weight', 'layers.2.mlp.dense_4h_to_h.bias', 'layers.13.input_layernorm.bias', 'layers.3.attention.dense.bias', 'layers.4.mlp.dense_h_to_4h.bias', 'layers.1.mlp.dense_h_to_4h.weight', 'layers.2.mlp.dense_4h_to_h.weight', 'layers.10.mlp.dense_4h_to_h.bias', 'layers.5.input_layernorm.weight', 'layers.5.input_layernorm.bias', 'layers.7.input_layernorm.weight', 'layers.6.post_attention_layernorm.bias', 'layers.10.input_layernorm.bias', 'layers.13.attention.query_key_value.weight', 'layers.15.mlp.dense_h_to_4h.weight', 'layers.15.attention.dense.weight', 'layers.6.attention.query_key_value.bias', 'layers.13.post_attention_layernorm.bias', 'layers.8.mlp.dense_4h_to_h.weight', 'layers.12.post_attention_layernorm.bias', 'layers.4.post_attention_layernorm.weight', 'layers.4.input_layernorm.weight', 'layers.5.mlp.dense_4h_to_h.bias', 'layers.1.input_layernorm.weight', 'layers.1.attention.query_key_value.bias', 'layers.6.input_layernorm.weight', 'layers.4.post_attention_layernorm.bias', 'layers.9.input_layernorm.weight', 'layers.4.input_layernorm.bias', 'layers.12.input_layernorm.weight', 'layers.13.attention.query_key_value.bias', 'layers.13.mlp.dense_h_to_4h.weight', 'layers.0.attention.query_key_value.bias', 'layers.15.mlp.dense_4h_to_h.bias', 'layers.3.mlp.dense_4h_to_h.bias', 'layers.0.mlp.dense_h_to_4h.weight', 'final_layer_norm.weight', 'layers.11.attention.dense.weight', 'layers.14.input_layernorm.weight', 'layers.1.attention.dense.bias', 'layers.12.post_attention_layernorm.weight', 'layers.1.attention.query_key_value.weight', 'layers.7.attention.query_key_value.weight', 'layers.14.mlp.dense_h_to_4h.bias', 'layers.4.attention.query_key_value.weight', 'layers.11.mlp.dense_h_to_4h.bias', 'layers.14.attention.query_key_value.bias', 'layers.8.mlp.dense_h_to_4h.weight', 'layers.9.attention.dense.bias', 'layers.5.attention.query_key_value.weight', 'layers.7.mlp.dense_4h_to_h.bias', 'layers.9.mlp.dense_h_to_4h.weight', 'layers.13.input_layernorm.weight', 'layers.9.post_attention_layernorm.weight', 'layers.8.post_attention_layernorm.bias', 'layers.15.mlp.dense_h_to_4h.bias', 'layers.6.attention.dense.bias', 'layers.5.attention.query_key_value.bias', 'layers.7.attention.dense.weight', 'embed_in.weight', 'layers.9.attention.query_key_value.bias', 'layers.7.post_attention_layernorm.bias', 'layers.9.mlp.dense_4h_to_h.bias', 'layers.11.post_attention_layernorm.bias', 'layers.5.mlp.dense_4h_to_h.weight', 'layers.10.mlp.dense_4h_to_h.weight', 'layers.3.post_attention_layernorm.weight', 'layers.15.post_attention_layernorm.bias', 'layers.8.input_layernorm.bias', 'layers.8.attention.dense.weight', 'layers.11.mlp.dense_4h_to_h.bias', 'layers.10.post_attention_layernorm.bias', 'layers.2.mlp.dense_h_to_4h.weight', 'layers.3.attention.query_key_value.weight', 'layers.13.attention.dense.bias', 'layers.7.attention.dense.bias', 'layers.13.attention.dense.weight', 'layers.0.post_attention_layernorm.bias', 'layers.9.attention.query_key_value.weight', 'layers.2.post_attention_layernorm.weight', 'layers.14.mlp.dense_h_to_4h.weight', 'layers.1.input_layernorm.bias', 'layers.6.attention.dense.weight', 'layers.9.input_layernorm.bias', 'layers.13.mlp.dense_h_to_4h.bias', 'layers.1.mlp.dense_4h_to_h.weight', 'layers.5.mlp.dense_h_to_4h.bias', 'layers.8.mlp.dense_h_to_4h.bias', 'layers.11.mlp.dense_4h_to_h.weight', 'layers.7.input_layernorm.bias', 'layers.5.attention.dense.bias', 'layers.0.mlp.dense_4h_to_h.bias', 'layers.15.mlp.dense_4h_to_h.weight', 'final_layer_norm.bias', 'layers.12.mlp.dense_4h_to_h.weight', 'layers.15.input_layernorm.weight', 'layers.6.input_layernorm.bias', 'layers.1.mlp.dense_4h_to_h.bias', 'layers.3.attention.query_key_value.bias', 'layers.5.attention.dense.weight', 'layers.4.attention.query_key_value.bias', 'layers.7.attention.query_key_value.bias', 'layers.5.mlp.dense_h_to_4h.weight', 'layers.7.mlp.dense_h_to_4h.weight', 'layers.8.post_attention_layernorm.weight', 'layers.0.input_layernorm.weight', 'layers.12.attention.dense.weight', 'layers.12.attention.query_key_value.bias', 'layers.6.post_attention_layernorm.weight', 'layers.10.mlp.dense_h_to_4h.weight', 'layers.3.post_attention_layernorm.bias', 'layers.4.attention.dense.weight', 'layers.8.mlp.dense_4h_to_h.bias', 'layers.3.input_layernorm.bias', 'layers.6.mlp.dense_h_to_4h.bias', 'layers.11.attention.query_key_value.bias', 'layers.14.attention.dense.bias', 'layers.6.attention.query_key_value.weight', 'layers.4.mlp.dense_4h_to_h.bias', 'layers.4.mlp.dense_4h_to_h.weight', 'layers.11.input_layernorm.bias', 'layers.6.mlp.dense_4h_to_h.bias', 'layers.12.mlp.dense_4h_to_h.bias', 'layers.11.attention.query_key_value.weight', 'layers.9.mlp.dense_4h_to_h.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs were not used when initializing GPTNeoXForCausalLM: ['base_model.gpt_neox.layers.5.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.2.input_layernorm.bias', 'base_model.gpt_neox.layers.15.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.2.attention.dense.bias', 'base_model.gpt_neox.layers.0.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.11.attention.dense.weight', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.4.attention.query_key_value.bias', 'base_model.gpt_neox.layers.1.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.3.attention.query_key_value.bias', 'frozen_head.final_norm.bias', 'v_head.2.weight', 'frozen_head.decoder_blocks.1.attention.dense.bias', 'base_model.gpt_neox.layers.12.attention.query_key_value.bias', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.12.input_layernorm.weight', 'base_model.gpt_neox.layers.7.attention.query_key_value.weight', 'frozen_head.decoder_blocks.1.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.0.attention.query_key_value.bias', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.5.attention.query_key_value.weight', 'frozen_head.decoder_blocks.1.input_layernorm.bias', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.2.attention.query_key_value.bias', 'base_model.gpt_neox.layers.15.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.13.input_layernorm.bias', 'frozen_head.decoder_blocks.0.input_layernorm.weight', 'base_model.gpt_neox.layers.1.attention.dense.bias', 'base_model.gpt_neox.layers.12.input_layernorm.bias', 'frozen_head.decoder_blocks.0.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.15.attention.dense.weight', 'base_model.gpt_neox.layers.0.input_layernorm.weight', 'frozen_head.decoder_blocks.1.attention.query_key_value.weight', 'base_model.gpt_neox.layers.4.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.6.input_layernorm.bias', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.0.input_layernorm.bias', 'frozen_head.decoder_blocks.2.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.bias', 'v_head.0.bias', 'base_model.gpt_neox.layers.2.attention.dense.bias', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.3.attention.query_key_value.weight', 'frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.0.attention.dense.bias', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.13.attention.dense.weight', 'base_model.gpt_neox.final_layer_norm.bias', 'base_model.gpt_neox.layers.6.attention.dense.bias', 'base_model.gpt_neox.layers.8.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.10.attention.dense.weight', 'base_model.gpt_neox.layers.14.attention.dense.bias', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.14.attention.query_key_value.weight', 'base_model.gpt_neox.layers.14.input_layernorm.bias', 'base_model.gpt_neox.layers.9.attention.dense.weight', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.bias', 'v_head.0.weight', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.3.input_layernorm.bias', 'base_model.gpt_neox.layers.4.attention.query_key_value.weight', 'base_model.gpt_neox.layers.11.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.11.input_layernorm.bias', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.2.attention.dense.weight', 'base_model.gpt_neox.layers.10.attention.dense.bias', 'base_model.gpt_neox.layers.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.15.attention.query_key_value.weight', 'base_model.gpt_neox.layers.1.attention.query_key_value.bias', 'base_model.gpt_neox.layers.6.attention.query_key_value.weight', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.13.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.3.post_attention_layernorm.weight', 'v_head.2.bias', 'base_model.gpt_neox.layers.8.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.5.attention.query_key_value.bias', 'base_model.gpt_neox.layers.9.attention.query_key_value.bias', 'frozen_head.decoder_blocks.3.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.4.attention.dense.bias', 'frozen_head.final_norm.weight', 'frozen_head.decoder_blocks.0.attention.dense.bias', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.1.attention.query_key_value.weight', 'frozen_head.decoder_blocks.2.attention.query_key_value.weight', 'base_model.gpt_neox.layers.8.attention.dense.weight', 'base_model.gpt_neox.layers.10.input_layernorm.weight', 'frozen_head.decoder_blocks.0.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.5.attention.dense.weight', 'base_model.gpt_neox.layers.14.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.0.attention.dense.weight', 'base_model.gpt_neox.layers.12.attention.dense.bias', 'base_model.gpt_neox.layers.11.attention.query_key_value.weight', 'base_model.gpt_neox.layers.6.attention.dense.weight', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.2.attention.dense.weight', 'base_model.gpt_neox.layers.9.attention.query_key_value.weight', 'base_model.gpt_neox.layers.1.input_layernorm.weight', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.7.attention.dense.bias', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.9.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.3.attention.dense.weight', 'base_model.gpt_neox.layers.1.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.4.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.3.attention.query_key_value.weight', 'frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.2.input_layernorm.bias', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.3.attention.dense.weight', 'base_model.gpt_neox.layers.3.attention.dense.bias', 'base_model.gpt_neox.layers.9.input_layernorm.weight', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.9.input_layernorm.bias', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.5.input_layernorm.weight', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.7.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.11.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.15.input_layernorm.weight', 'base_model.gpt_neox.layers.2.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.9.attention.dense.bias', 'base_model.gpt_neox.layers.12.post_attention_layernorm.bias', 'base_model.embed_out.weight', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.3.input_layernorm.bias', 'base_model.gpt_neox.layers.8.attention.query_key_value.bias', 'base_model.gpt_neox.layers.4.attention.dense.weight', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.1.input_layernorm.bias', 'frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.0.attention.query_key_value.bias', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.2.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.3.attention.dense.bias', 'base_model.gpt_neox.layers.12.attention.dense.weight', 'base_model.gpt_neox.layers.14.attention.query_key_value.bias', 'base_model.gpt_neox.layers.10.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.11.attention.query_key_value.bias', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.weight', 'frozen_head.lm_head.weight', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.15.input_layernorm.bias', 'base_model.gpt_neox.layers.10.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.5.input_layernorm.bias', 'base_model.gpt_neox.layers.10.attention.query_key_value.bias', 'base_model.gpt_neox.layers.13.attention.query_key_value.bias', 'base_model.gpt_neox.layers.3.attention.query_key_value.bias', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.7.input_layernorm.weight', 'base_model.gpt_neox.layers.4.input_layernorm.bias', 'base_model.gpt_neox.layers.13.attention.dense.bias', 'base_model.gpt_neox.layers.10.attention.query_key_value.weight', 'base_model.gpt_neox.layers.4.input_layernorm.weight', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.2.input_layernorm.weight', 'base_model.gpt_neox.layers.11.input_layernorm.weight', 'base_model.gpt_neox.layers.3.input_layernorm.weight', 'frozen_head.decoder_blocks.1.attention.query_key_value.bias', 'base_model.gpt_neox.final_layer_norm.weight', 'base_model.gpt_neox.layers.2.attention.query_key_value.bias', 'frozen_head.decoder_blocks.1.input_layernorm.weight', 'frozen_head.decoder_blocks.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.1.attention.dense.weight', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.2.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.13.attention.query_key_value.weight', 'base_model.gpt_neox.layers.12.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.0.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.13.input_layernorm.weight', 'base_model.gpt_neox.layers.5.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.8.input_layernorm.bias', 'base_model.gpt_neox.layers.8.input_layernorm.weight', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.11.attention.dense.bias', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.2.input_layernorm.weight', 'base_model.gpt_neox.layers.6.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.0.attention.query_key_value.weight', 'base_model.gpt_neox.layers.6.input_layernorm.weight', 'base_model.gpt_neox.layers.6.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.14.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.5.attention.dense.bias', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.14.attention.dense.weight', 'base_model.gpt_neox.layers.7.attention.query_key_value.bias', 'base_model.gpt_neox.layers.7.input_layernorm.bias', 'base_model.gpt_neox.layers.9.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.1.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.10.input_layernorm.bias', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.14.input_layernorm.weight', 'base_model.gpt_neox.layers.0.input_layernorm.bias', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.15.attention.dense.bias', 'frozen_head.decoder_blocks.1.attention.dense.weight', 'base_model.gpt_neox.layers.6.attention.query_key_value.bias', 'base_model.gpt_neox.layers.7.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.2.attention.query_key_value.weight', 'base_model.gpt_neox.layers.15.attention.query_key_value.bias', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.12.attention.query_key_value.weight', 'base_model.gpt_neox.layers.13.post_attention_layernorm.weight', 'base_model.gpt_neox.embed_in.weight', 'frozen_head.decoder_blocks.3.input_layernorm.weight', 'base_model.gpt_neox.layers.8.attention.dense.bias', 'base_model.gpt_neox.layers.7.attention.dense.weight', 'frozen_head.decoder_blocks.0.attention.dense.weight', 'base_model.gpt_neox.layers.0.attention.query_key_value.weight', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.8.attention.query_key_value.weight']
- This IS expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs and are newly initialized: ['layers.0.attention.query_key_value.bias', 'layers.1.attention.query_key_value.weight', 'layers.15.post_attention_layernorm.bias', 'layers.1.mlp.dense_4h_to_h.weight', 'layers.4.post_attention_layernorm.weight', 'layers.0.mlp.dense_h_to_4h.bias', 'layers.5.mlp.dense_4h_to_h.weight', 'layers.15.attention.query_key_value.bias', 'layers.4.mlp.dense_4h_to_h.weight', 'layers.9.attention.dense.bias', 'layers.6.input_layernorm.bias', 'layers.13.input_layernorm.bias', 'layers.2.input_layernorm.weight', 'layers.8.attention.query_key_value.bias', 'layers.2.attention.dense.weight', 'layers.10.mlp.dense_4h_to_h.weight', 'layers.11.mlp.dense_4h_to_h.bias', 'layers.2.attention.query_key_value.bias', 'layers.2.mlp.dense_4h_to_h.weight', 'layers.4.attention.dense.bias', 'layers.15.attention.dense.weight', 'layers.7.input_layernorm.bias', 'layers.0.post_attention_layernorm.bias', 'layers.3.mlp.dense_4h_to_h.weight', 'layers.9.input_layernorm.weight', 'layers.10.post_attention_layernorm.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.post_attention_layernorm.bias', 'layers.5.input_layernorm.bias', 'layers.3.input_layernorm.bias', 'layers.6.attention.query_key_value.weight', 'layers.12.mlp.dense_h_to_4h.weight', 'layers.1.post_attention_layernorm.weight', 'layers.8.mlp.dense_4h_to_h.bias', 'layers.1.mlp.dense_4h_to_h.bias', 'layers.0.attention.dense.bias', 'layers.5.post_attention_layernorm.weight', 'layers.6.mlp.dense_4h_to_h.weight', 'layers.11.input_layernorm.bias', 'layers.10.attention.dense.weight', 'layers.11.attention.query_key_value.bias', 'layers.8.attention.dense.weight', 'layers.15.mlp.dense_4h_to_h.bias', 'layers.14.input_layernorm.bias', 'layers.13.attention.query_key_value.bias', 'layers.10.attention.query_key_value.bias', 'layers.4.mlp.dense_4h_to_h.bias', 'layers.14.attention.dense.weight', 'layers.2.attention.dense.bias', 'layers.9.mlp.dense_h_to_4h.bias', 'layers.12.attention.query_key_value.bias', 'layers.9.attention.query_key_value.bias', 'layers.12.input_layernorm.weight', 'layers.5.mlp.dense_4h_to_h.bias', 'layers.0.input_layernorm.weight', 'layers.5.input_layernorm.weight', 'layers.10.mlp.dense_h_to_4h.bias', 'layers.13.attention.dense.bias', 'layers.14.attention.query_key_value.bias', 'layers.6.mlp.dense_4h_to_h.bias', 'layers.8.mlp.dense_h_to_4h.weight', 'layers.10.input_layernorm.weight', 'layers.13.post_attention_layernorm.bias', 'layers.1.input_layernorm.bias', 'layers.3.attention.dense.bias', 'embed_in.weight', 'layers.10.mlp.dense_h_to_4h.weight', 'layers.13.attention.query_key_value.weight', 'layers.0.post_attention_layernorm.weight', 'layers.2.attention.query_key_value.weight', 'layers.5.attention.query_key_value.bias', 'layers.4.input_layernorm.bias', 'layers.4.input_layernorm.weight', 'layers.2.mlp.dense_h_to_4h.bias', 'layers.7.post_attention_layernorm.weight', 'layers.8.input_layernorm.bias', 'layers.8.input_layernorm.weight', 'layers.7.mlp.dense_4h_to_h.weight', 'layers.0.attention.dense.weight', 'layers.6.attention.dense.weight', 'layers.7.attention.dense.weight', 'layers.8.attention.query_key_value.weight', 'layers.1.mlp.dense_h_to_4h.weight', 'layers.6.post_attention_layernorm.weight', 'layers.12.mlp.dense_4h_to_h.weight', 'layers.5.attention.dense.weight', 'layers.11.post_attention_layernorm.weight', 'layers.13.mlp.dense_4h_to_h.weight', 'layers.2.post_attention_layernorm.weight', 'layers.3.mlp.dense_h_to_4h.weight', 'layers.6.mlp.dense_h_to_4h.bias', 'layers.8.mlp.dense_h_to_4h.bias', 'layers.6.attention.query_key_value.bias', 'layers.12.attention.dense.weight', 'layers.1.attention.query_key_value.bias', 'layers.2.mlp.dense_4h_to_h.bias', 'layers.11.input_layernorm.weight', 'layers.7.input_layernorm.weight', 'layers.11.attention.dense.weight', 'layers.12.attention.dense.bias', 'layers.6.attention.dense.bias', 'layers.15.mlp.dense_4h_to_h.weight', 'layers.2.input_layernorm.bias', 'layers.13.mlp.dense_h_to_4h.weight', 'layers.3.attention.dense.weight', 'layers.13.attention.dense.weight', 'layers.7.attention.query_key_value.bias', 'layers.14.mlp.dense_4h_to_h.bias', 'layers.7.attention.query_key_value.weight', 'layers.11.attention.query_key_value.weight', 'layers.5.post_attention_layernorm.bias', 'layers.11.mlp.dense_h_to_4h.weight', 'final_layer_norm.weight', 'layers.1.input_layernorm.weight', 'layers.5.attention.dense.bias', 'layers.6.mlp.dense_h_to_4h.weight', 'layers.8.attention.dense.bias', 'layers.13.mlp.dense_h_to_4h.bias', 'layers.1.mlp.dense_h_to_4h.bias', 'layers.1.attention.dense.bias', 'layers.4.attention.query_key_value.weight', 'layers.15.input_layernorm.bias', 'layers.15.attention.query_key_value.weight', 'layers.7.mlp.dense_h_to_4h.weight', 'layers.12.post_attention_layernorm.weight', 'layers.15.mlp.dense_h_to_4h.weight', 'layers.11.mlp.dense_h_to_4h.bias', 'layers.4.attention.dense.weight', 'layers.14.mlp.dense_h_to_4h.weight', 'layers.3.post_attention_layernorm.weight', 'layers.12.post_attention_layernorm.bias', 'layers.11.attention.dense.bias', 'layers.13.input_layernorm.weight', 'layers.9.input_layernorm.bias', 'layers.3.mlp.dense_h_to_4h.bias', 'layers.6.post_attention_layernorm.bias', 'layers.3.mlp.dense_4h_to_h.bias', 'layers.9.mlp.dense_4h_to_h.weight', 'layers.10.attention.query_key_value.weight', 'layers.5.mlp.dense_h_to_4h.bias', 'layers.12.attention.query_key_value.weight', 'layers.15.post_attention_layernorm.weight', 'layers.14.mlp.dense_4h_to_h.weight', 'layers.12.mlp.dense_4h_to_h.bias', 'layers.7.attention.dense.bias', 'layers.1.attention.dense.weight', 'layers.4.mlp.dense_h_to_4h.weight', 'layers.9.post_attention_layernorm.bias', 'layers.4.attention.query_key_value.bias', 'layers.14.mlp.dense_h_to_4h.bias', 'layers.7.mlp.dense_4h_to_h.bias', 'layers.15.mlp.dense_h_to_4h.bias', 'layers.12.input_layernorm.bias', 'layers.9.attention.dense.weight', 'layers.0.input_layernorm.bias', 'layers.4.mlp.dense_h_to_4h.bias', 'layers.14.attention.dense.bias', 'layers.14.post_attention_layernorm.weight', 'layers.0.attention.query_key_value.weight', 'layers.13.mlp.dense_4h_to_h.bias', 'layers.2.post_attention_layernorm.bias', 'layers.0.mlp.dense_4h_to_h.bias', 'layers.4.post_attention_layernorm.bias', 'layers.3.post_attention_layernorm.bias', 'layers.11.mlp.dense_4h_to_h.weight', 'final_layer_norm.bias', 'layers.13.post_attention_layernorm.weight', 'layers.6.input_layernorm.weight', 'layers.14.input_layernorm.weight', 'layers.10.attention.dense.bias', 'layers.5.mlp.dense_h_to_4h.weight', 'layers.7.mlp.dense_h_to_4h.bias', 'layers.2.mlp.dense_h_to_4h.weight', 'layers.10.post_attention_layernorm.bias', 'layers.9.post_attention_layernorm.weight', 'layers.0.mlp.dense_h_to_4h.weight', 'layers.15.input_layernorm.weight', 'layers.7.post_attention_layernorm.bias', 'layers.1.post_attention_layernorm.bias', 'layers.0.mlp.dense_4h_to_h.weight', 'layers.3.attention.query_key_value.bias', 'layers.14.attention.query_key_value.weight', 'layers.5.attention.query_key_value.weight', 'layers.8.mlp.dense_4h_to_h.weight', 'layers.15.attention.dense.bias', 'embed_out.weight', 'layers.12.mlp.dense_h_to_4h.bias', 'layers.10.input_layernorm.bias', 'layers.3.input_layernorm.weight', 'layers.9.attention.query_key_value.weight', 'layers.9.mlp.dense_h_to_4h.weight', 'layers.10.mlp.dense_4h_to_h.bias', 'layers.3.attention.query_key_value.weight', 'layers.14.post_attention_layernorm.bias', 'layers.11.post_attention_layernorm.bias', 'layers.9.mlp.dense_4h_to_h.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs were not used when initializing GPTNeoXForCausalLM: ['frozen_head.decoder_blocks.1.attention.dense.bias', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.3.attention.dense.weight', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.0.attention.dense.weight', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.9.post_attention_layernorm.bias', 'frozen_head.final_norm.weight', 'base_model.gpt_neox.layers.0.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.3.attention.dense.weight', 'base_model.gpt_neox.layers.8.input_layernorm.bias', 'base_model.gpt_neox.layers.9.attention.dense.bias', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.11.attention.query_key_value.bias', 'base_model.gpt_neox.layers.11.input_layernorm.weight', 'base_model.gpt_neox.layers.6.attention.query_key_value.weight', 'base_model.gpt_neox.layers.4.input_layernorm.bias', 'frozen_head.decoder_blocks.3.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.3.attention.dense.bias', 'base_model.gpt_neox.layers.14.input_layernorm.bias', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.2.input_layernorm.weight', 'base_model.gpt_neox.layers.3.input_layernorm.weight', 'base_model.gpt_neox.layers.2.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.7.attention.query_key_value.bias', 'base_model.gpt_neox.layers.0.input_layernorm.weight', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.3.input_layernorm.weight', 'base_model.gpt_neox.layers.6.attention.dense.weight', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.attention.query_key_value.weight', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.12.input_layernorm.weight', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.8.attention.query_key_value.bias', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.3.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.8.input_layernorm.weight', 'base_model.gpt_neox.layers.0.attention.query_key_value.weight', 'base_model.gpt_neox.layers.7.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.5.attention.dense.weight', 'base_model.gpt_neox.layers.6.attention.query_key_value.bias', 'base_model.gpt_neox.layers.11.attention.dense.weight', 'base_model.gpt_neox.layers.2.attention.dense.bias', 'base_model.gpt_neox.layers.9.input_layernorm.weight', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.0.input_layernorm.bias', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.14.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.5.input_layernorm.bias', 'base_model.gpt_neox.layers.15.input_layernorm.bias', 'base_model.gpt_neox.layers.7.attention.dense.bias', 'frozen_head.decoder_blocks.0.input_layernorm.bias', 'frozen_head.decoder_blocks.1.attention.query_key_value.weight', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.3.attention.query_key_value.bias', 'base_model.gpt_neox.layers.9.attention.query_key_value.weight', 'base_model.gpt_neox.layers.12.input_layernorm.bias', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.1.attention.dense.bias', 'base_model.gpt_neox.layers.10.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.14.attention.query_key_value.bias', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.0.attention.dense.bias', 'base_model.gpt_neox.layers.2.attention.query_key_value.bias', 'base_model.gpt_neox.layers.15.attention.query_key_value.bias', 'base_model.gpt_neox.layers.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.0.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.11.post_attention_layernorm.weight', 'base_model.gpt_neox.final_layer_norm.weight', 'base_model.gpt_neox.layers.14.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.7.input_layernorm.weight', 'frozen_head.decoder_blocks.0.attention.dense.bias', 'base_model.gpt_neox.layers.7.attention.dense.weight', 'base_model.gpt_neox.layers.3.attention.dense.bias', 'base_model.gpt_neox.layers.10.attention.query_key_value.bias', 'base_model.gpt_neox.layers.6.input_layernorm.weight', 'base_model.gpt_neox.layers.4.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.1.attention.query_key_value.bias', 'base_model.gpt_neox.layers.6.attention.dense.bias', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.5.input_layernorm.weight', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.final_layer_norm.bias', 'base_model.gpt_neox.layers.11.attention.dense.bias', 'base_model.gpt_neox.layers.13.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.11.input_layernorm.bias', 'base_model.gpt_neox.layers.11.attention.query_key_value.weight', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.0.attention.query_key_value.bias', 'base_model.gpt_neox.layers.6.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.0.post_attention_layernorm.weight', 'v_head.0.bias', 'base_model.gpt_neox.layers.14.attention.query_key_value.weight', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.embed_in.weight', 'base_model.gpt_neox.layers.12.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.10.input_layernorm.weight', 'base_model.gpt_neox.layers.4.attention.query_key_value.weight', 'base_model.gpt_neox.layers.4.attention.dense.bias', 'base_model.gpt_neox.layers.11.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.12.attention.query_key_value.bias', 'frozen_head.decoder_blocks.3.attention.query_key_value.bias', 'base_model.gpt_neox.layers.13.input_layernorm.bias', 'frozen_head.decoder_blocks.2.input_layernorm.weight', 'base_model.gpt_neox.layers.1.attention.query_key_value.weight', 'base_model.gpt_neox.layers.1.attention.dense.weight', 'frozen_head.decoder_blocks.0.attention.query_key_value.weight', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.1.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.13.input_layernorm.weight', 'base_model.gpt_neox.layers.13.attention.query_key_value.bias', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.weight', 'base_model.embed_out.weight', 'base_model.gpt_neox.layers.10.attention.dense.weight', 'base_model.gpt_neox.layers.12.attention.dense.bias', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.4.input_layernorm.weight', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.2.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.15.attention.query_key_value.weight', 'frozen_head.decoder_blocks.1.input_layernorm.weight', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.9.post_attention_layernorm.weight', 'v_head.2.weight', 'base_model.gpt_neox.layers.14.attention.dense.bias', 'frozen_head.decoder_blocks.2.attention.dense.bias', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.0.input_layernorm.weight', 'base_model.gpt_neox.layers.6.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.2.attention.dense.weight', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.0.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.7.attention.query_key_value.weight', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.1.attention.query_key_value.bias', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.13.attention.query_key_value.weight', 'base_model.gpt_neox.layers.1.input_layernorm.bias', 'base_model.gpt_neox.layers.13.post_attention_layernorm.weight', 'v_head.0.weight', 'base_model.gpt_neox.layers.10.attention.query_key_value.weight', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.8.attention.dense.bias', 'frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.2.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.2.attention.dense.weight', 'base_model.gpt_neox.layers.12.attention.query_key_value.weight', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.1.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.15.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.4.attention.dense.weight', 'base_model.gpt_neox.layers.6.input_layernorm.bias', 'base_model.gpt_neox.layers.5.attention.query_key_value.bias', 'frozen_head.decoder_blocks.0.attention.query_key_value.bias', 'base_model.gpt_neox.layers.15.attention.dense.bias', 'base_model.gpt_neox.layers.9.attention.dense.weight', 'base_model.gpt_neox.layers.4.attention.query_key_value.bias', 'base_model.gpt_neox.layers.10.input_layernorm.bias', 'frozen_head.decoder_blocks.1.attention.dense.weight', 'base_model.gpt_neox.layers.5.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.1.input_layernorm.bias', 'base_model.gpt_neox.layers.1.input_layernorm.weight', 'frozen_head.decoder_blocks.1.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.5.attention.dense.bias', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.2.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.7.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.2.input_layernorm.bias', 'base_model.gpt_neox.layers.8.attention.dense.weight', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.5.attention.query_key_value.weight', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.10.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.9.input_layernorm.bias', 'frozen_head.decoder_blocks.2.attention.query_key_value.bias', 'frozen_head.final_norm.bias', 'base_model.gpt_neox.layers.15.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.10.attention.dense.bias', 'base_model.gpt_neox.layers.7.input_layernorm.bias', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.bias', 'v_head.2.bias', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.13.attention.dense.weight', 'base_model.gpt_neox.layers.12.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.8.attention.query_key_value.weight', 'base_model.gpt_neox.layers.2.input_layernorm.bias', 'frozen_head.decoder_blocks.3.attention.query_key_value.weight', 'base_model.gpt_neox.layers.1.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.8.post_attention_layernorm.weight', 'frozen_head.lm_head.weight', 'base_model.gpt_neox.layers.14.input_layernorm.weight', 'base_model.gpt_neox.layers.5.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.2.attention.query_key_value.weight', 'frozen_head.decoder_blocks.0.attention.dense.weight', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.12.attention.dense.weight', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.15.attention.dense.weight', 'frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.3.input_layernorm.bias', 'base_model.gpt_neox.layers.14.attention.dense.weight', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.9.attention.query_key_value.bias', 'base_model.gpt_neox.layers.8.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.3.input_layernorm.bias', 'base_model.gpt_neox.layers.15.input_layernorm.weight', 'base_model.gpt_neox.layers.2.attention.query_key_value.weight', 'base_model.gpt_neox.layers.13.attention.dense.bias', 'base_model.gpt_neox.layers.4.post_attention_layernorm.bias']
- This IS expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs and are newly initialized: ['layers.13.attention.query_key_value.bias', 'layers.0.mlp.dense_h_to_4h.weight', 'layers.0.attention.dense.bias', 'layers.2.post_attention_layernorm.bias', 'layers.3.mlp.dense_h_to_4h.bias', 'layers.6.mlp.dense_4h_to_h.weight', 'layers.11.attention.query_key_value.weight', 'layers.13.attention.dense.bias', 'layers.6.attention.dense.weight', 'layers.13.mlp.dense_h_to_4h.bias', 'layers.3.mlp.dense_4h_to_h.bias', 'layers.3.attention.dense.weight', 'layers.10.post_attention_layernorm.bias', 'layers.11.attention.dense.weight', 'layers.9.input_layernorm.bias', 'layers.11.attention.query_key_value.bias', 'layers.2.input_layernorm.weight', 'layers.1.input_layernorm.bias', 'layers.8.attention.query_key_value.bias', 'layers.5.post_attention_layernorm.bias', 'layers.10.attention.dense.bias', 'layers.10.mlp.dense_4h_to_h.bias', 'layers.4.attention.query_key_value.bias', 'layers.0.input_layernorm.weight', 'layers.9.attention.dense.weight', 'layers.3.attention.query_key_value.weight', 'final_layer_norm.bias', 'layers.10.input_layernorm.weight', 'layers.3.mlp.dense_4h_to_h.weight', 'layers.3.attention.query_key_value.bias', 'layers.6.attention.dense.bias', 'layers.5.input_layernorm.weight', 'layers.14.post_attention_layernorm.weight', 'layers.2.attention.query_key_value.bias', 'layers.6.attention.query_key_value.bias', 'layers.11.mlp.dense_h_to_4h.bias', 'embed_out.weight', 'layers.11.mlp.dense_h_to_4h.weight', 'layers.14.post_attention_layernorm.bias', 'layers.4.attention.query_key_value.weight', 'layers.6.mlp.dense_h_to_4h.weight', 'layers.5.mlp.dense_4h_to_h.weight', 'layers.7.attention.query_key_value.weight', 'layers.2.attention.query_key_value.weight', 'layers.3.input_layernorm.weight', 'layers.5.mlp.dense_h_to_4h.weight', 'layers.0.attention.query_key_value.bias', 'layers.1.mlp.dense_h_to_4h.bias', 'layers.9.attention.query_key_value.weight', 'layers.10.input_layernorm.bias', 'layers.11.attention.dense.bias', 'layers.15.mlp.dense_h_to_4h.bias', 'layers.2.mlp.dense_4h_to_h.bias', 'layers.13.mlp.dense_h_to_4h.weight', 'layers.12.mlp.dense_h_to_4h.weight', 'layers.5.post_attention_layernorm.weight', 'layers.0.post_attention_layernorm.weight', 'layers.14.attention.dense.bias', 'layers.9.attention.dense.bias', 'layers.7.attention.dense.bias', 'layers.13.input_layernorm.weight', 'layers.9.input_layernorm.weight', 'layers.0.post_attention_layernorm.bias', 'layers.7.attention.query_key_value.bias', 'layers.14.mlp.dense_h_to_4h.weight', 'layers.5.attention.dense.weight', 'layers.10.mlp.dense_4h_to_h.weight', 'layers.15.input_layernorm.bias', 'layers.4.attention.dense.weight', 'layers.8.attention.dense.bias', 'layers.10.mlp.dense_h_to_4h.weight', 'layers.11.post_attention_layernorm.weight', 'layers.5.input_layernorm.bias', 'layers.12.attention.dense.bias', 'layers.14.attention.dense.weight', 'layers.6.input_layernorm.weight', 'layers.9.post_attention_layernorm.weight', 'layers.7.post_attention_layernorm.weight', 'layers.10.mlp.dense_h_to_4h.bias', 'layers.12.post_attention_layernorm.weight', 'layers.15.post_attention_layernorm.weight', 'layers.7.attention.dense.weight', 'layers.15.attention.dense.weight', 'layers.4.post_attention_layernorm.weight', 'layers.5.attention.dense.bias', 'layers.4.mlp.dense_4h_to_h.bias', 'layers.0.mlp.dense_4h_to_h.weight', 'layers.12.input_layernorm.bias', 'layers.6.post_attention_layernorm.bias', 'layers.6.mlp.dense_h_to_4h.bias', 'layers.14.mlp.dense_h_to_4h.bias', 'layers.12.post_attention_layernorm.bias', 'layers.2.attention.dense.weight', 'layers.7.mlp.dense_h_to_4h.weight', 'final_layer_norm.weight', 'layers.15.mlp.dense_4h_to_h.bias', 'layers.0.input_layernorm.bias', 'layers.12.attention.query_key_value.bias', 'layers.3.mlp.dense_h_to_4h.weight', 'layers.10.attention.dense.weight', 'layers.1.input_layernorm.weight', 'layers.9.attention.query_key_value.bias', 'layers.4.attention.dense.bias', 'layers.1.mlp.dense_h_to_4h.weight', 'layers.3.input_layernorm.bias', 'layers.8.mlp.dense_h_to_4h.bias', 'layers.12.attention.dense.weight', 'layers.8.mlp.dense_4h_to_h.weight', 'layers.1.mlp.dense_4h_to_h.weight', 'layers.13.mlp.dense_4h_to_h.weight', 'layers.11.input_layernorm.weight', 'layers.4.mlp.dense_4h_to_h.weight', 'layers.7.mlp.dense_4h_to_h.weight', 'layers.7.post_attention_layernorm.bias', 'layers.11.mlp.dense_4h_to_h.weight', 'layers.14.input_layernorm.weight', 'layers.6.input_layernorm.bias', 'layers.11.input_layernorm.bias', 'layers.12.attention.query_key_value.weight', 'layers.11.mlp.dense_4h_to_h.bias', 'layers.15.attention.dense.bias', 'layers.0.attention.dense.weight', 'layers.15.mlp.dense_4h_to_h.weight', 'layers.15.attention.query_key_value.bias', 'layers.8.attention.dense.weight', 'layers.2.mlp.dense_4h_to_h.weight', 'layers.9.mlp.dense_4h_to_h.bias', 'layers.9.mlp.dense_h_to_4h.weight', 'layers.12.mlp.dense_4h_to_h.bias', 'layers.14.mlp.dense_4h_to_h.weight', 'layers.13.attention.dense.weight', 'embed_in.weight', 'layers.2.attention.dense.bias', 'layers.7.input_layernorm.weight', 'layers.14.attention.query_key_value.bias', 'layers.3.attention.dense.bias', 'layers.2.input_layernorm.bias', 'layers.4.mlp.dense_h_to_4h.bias', 'layers.1.attention.query_key_value.weight', 'layers.1.post_attention_layernorm.bias', 'layers.8.attention.query_key_value.weight', 'layers.11.post_attention_layernorm.bias', 'layers.2.mlp.dense_h_to_4h.bias', 'layers.3.post_attention_layernorm.bias', 'layers.13.attention.query_key_value.weight', 'layers.13.post_attention_layernorm.weight', 'layers.10.attention.query_key_value.bias', 'layers.7.input_layernorm.bias', 'layers.9.post_attention_layernorm.bias', 'layers.4.post_attention_layernorm.bias', 'layers.5.attention.query_key_value.bias', 'layers.1.attention.query_key_value.bias', 'layers.8.input_layernorm.bias', 'layers.1.post_attention_layernorm.weight', 'layers.0.mlp.dense_h_to_4h.bias', 'layers.13.post_attention_layernorm.bias', 'layers.15.post_attention_layernorm.bias', 'layers.10.post_attention_layernorm.weight', 'layers.8.post_attention_layernorm.bias', 'layers.0.mlp.dense_4h_to_h.bias', 'layers.4.mlp.dense_h_to_4h.weight', 'layers.1.attention.dense.weight', 'layers.4.input_layernorm.bias', 'layers.13.input_layernorm.bias', 'layers.14.attention.query_key_value.weight', 'layers.7.mlp.dense_4h_to_h.bias', 'layers.12.input_layernorm.weight', 'layers.8.mlp.dense_h_to_4h.weight', 'layers.0.attention.query_key_value.weight', 'layers.1.attention.dense.bias', 'layers.2.post_attention_layernorm.weight', 'layers.9.mlp.dense_h_to_4h.bias', 'layers.15.input_layernorm.weight', 'layers.6.attention.query_key_value.weight', 'layers.7.mlp.dense_h_to_4h.bias', 'layers.2.mlp.dense_h_to_4h.weight', 'layers.14.mlp.dense_4h_to_h.bias', 'layers.5.mlp.dense_h_to_4h.bias', 'layers.6.post_attention_layernorm.weight', 'layers.9.mlp.dense_4h_to_h.weight', 'layers.13.mlp.dense_4h_to_h.bias', 'layers.6.mlp.dense_4h_to_h.bias', 'layers.5.mlp.dense_4h_to_h.bias', 'layers.8.input_layernorm.weight', 'layers.12.mlp.dense_4h_to_h.weight', 'layers.3.post_attention_layernorm.weight', 'layers.1.mlp.dense_4h_to_h.bias', 'layers.4.input_layernorm.weight', 'layers.15.mlp.dense_h_to_4h.weight', 'layers.5.attention.query_key_value.weight', 'layers.8.post_attention_layernorm.weight', 'layers.14.input_layernorm.bias', 'layers.15.attention.query_key_value.weight', 'layers.12.mlp.dense_h_to_4h.bias', 'layers.8.mlp.dense_4h_to_h.bias', 'layers.10.attention.query_key_value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs were not used when initializing GPTNeoXForCausalLM: ['v_head.2.weight', 'base_model.gpt_neox.layers.0.attention.query_key_value.bias', 'frozen_head.decoder_blocks.0.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.1.input_layernorm.weight', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.2.attention.query_key_value.weight', 'base_model.gpt_neox.layers.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.weight', 'v_head.0.bias', 'base_model.gpt_neox.layers.8.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.14.attention.dense.bias', 'base_model.gpt_neox.layers.12.attention.dense.weight', 'base_model.gpt_neox.layers.12.attention.dense.bias', 'base_model.gpt_neox.layers.6.input_layernorm.weight', 'frozen_head.decoder_blocks.3.attention.query_key_value.bias', 'base_model.gpt_neox.embed_in.weight', 'base_model.gpt_neox.layers.10.attention.query_key_value.weight', 'frozen_head.decoder_blocks.1.post_attention_layernorm.bias', 'v_head.2.bias', 'base_model.gpt_neox.final_layer_norm.bias', 'frozen_head.decoder_blocks.0.attention.query_key_value.weight', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.3.attention.dense.weight', 'base_model.gpt_neox.layers.8.attention.query_key_value.weight', 'base_model.gpt_neox.layers.0.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.11.input_layernorm.weight', 'base_model.gpt_neox.layers.4.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.4.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.4.attention.dense.bias', 'frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.10.input_layernorm.weight', 'base_model.gpt_neox.layers.3.input_layernorm.bias', 'base_model.gpt_neox.layers.9.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.14.input_layernorm.bias', 'base_model.embed_out.weight', 'frozen_head.decoder_blocks.2.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.1.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.0.attention.dense.bias', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.8.attention.query_key_value.bias', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.8.attention.dense.weight', 'base_model.gpt_neox.layers.14.attention.dense.weight', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.3.attention.dense.bias', 'frozen_head.decoder_blocks.3.attention.dense.weight', 'base_model.gpt_neox.layers.6.attention.dense.bias', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.bias', 'frozen_head.final_norm.bias', 'base_model.gpt_neox.layers.10.attention.query_key_value.bias', 'base_model.gpt_neox.layers.2.input_layernorm.weight', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.2.attention.dense.weight', 'base_model.gpt_neox.layers.8.input_layernorm.bias', 'frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.5.attention.query_key_value.bias', 'base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.2.attention.dense.bias', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.1.attention.query_key_value.bias', 'base_model.gpt_neox.layers.9.attention.query_key_value.bias', 'frozen_head.decoder_blocks.0.input_layernorm.weight', 'base_model.gpt_neox.layers.13.attention.dense.bias', 'base_model.gpt_neox.layers.3.attention.dense.bias', 'frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.1.attention.dense.bias', 'base_model.gpt_neox.layers.10.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.14.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.2.attention.dense.bias', 'base_model.gpt_neox.layers.9.attention.dense.weight', 'base_model.gpt_neox.layers.6.post_attention_layernorm.weight', 'frozen_head.final_norm.weight', 'base_model.gpt_neox.layers.12.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.2.attention.query_key_value.bias', 'frozen_head.decoder_blocks.0.input_layernorm.bias', 'frozen_head.lm_head.weight', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.final_layer_norm.weight', 'base_model.gpt_neox.layers.13.input_layernorm.bias', 'base_model.gpt_neox.layers.13.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.3.attention.query_key_value.weight', 'base_model.gpt_neox.layers.15.attention.dense.bias', 'base_model.gpt_neox.layers.5.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.2.attention.dense.weight', 'base_model.gpt_neox.layers.6.input_layernorm.bias', 'base_model.gpt_neox.layers.7.attention.query_key_value.bias', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.11.attention.query_key_value.bias', 'base_model.gpt_neox.layers.2.attention.query_key_value.bias', 'base_model.gpt_neox.layers.15.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.5.attention.dense.weight', 'base_model.gpt_neox.layers.7.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.8.attention.dense.bias', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.0.attention.query_key_value.weight', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.10.attention.dense.weight', 'base_model.gpt_neox.layers.12.input_layernorm.weight', 'base_model.gpt_neox.layers.14.input_layernorm.weight', 'base_model.gpt_neox.layers.11.attention.query_key_value.weight', 'base_model.gpt_neox.layers.2.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.13.attention.query_key_value.bias', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.15.input_layernorm.bias', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.10.attention.dense.bias', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.attention.query_key_value.bias', 'frozen_head.decoder_blocks.0.attention.query_key_value.bias', 'base_model.gpt_neox.layers.13.input_layernorm.weight', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.4.input_layernorm.bias', 'base_model.gpt_neox.layers.1.input_layernorm.weight', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.5.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.13.attention.query_key_value.weight', 'frozen_head.decoder_blocks.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.5.attention.query_key_value.weight', 'base_model.gpt_neox.layers.13.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.15.attention.query_key_value.bias', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.3.input_layernorm.weight', 'base_model.gpt_neox.layers.12.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.8.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.15.attention.query_key_value.weight', 'base_model.gpt_neox.layers.8.input_layernorm.weight', 'base_model.gpt_neox.layers.2.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.11.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.12.attention.query_key_value.bias', 'frozen_head.decoder_blocks.1.attention.query_key_value.weight', 'base_model.gpt_neox.layers.15.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.11.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.0.input_layernorm.weight', 'base_model.gpt_neox.layers.1.input_layernorm.bias', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.1.attention.dense.weight', 'base_model.gpt_neox.layers.7.input_layernorm.weight', 'base_model.gpt_neox.layers.0.attention.dense.weight', 'base_model.gpt_neox.layers.15.input_layernorm.weight', 'v_head.0.weight', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.9.attention.dense.bias', 'frozen_head.decoder_blocks.2.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.9.input_layernorm.weight', 'frozen_head.decoder_blocks.2.input_layernorm.weight', 'base_model.gpt_neox.layers.11.attention.dense.bias', 'base_model.gpt_neox.layers.10.input_layernorm.bias', 'base_model.gpt_neox.layers.2.attention.query_key_value.weight', 'base_model.gpt_neox.layers.6.attention.query_key_value.bias', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.11.input_layernorm.bias', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.4.attention.query_key_value.weight', 'base_model.gpt_neox.layers.5.attention.dense.bias', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.0.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.3.attention.query_key_value.weight', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.4.attention.dense.weight', 'base_model.gpt_neox.layers.1.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.0.attention.dense.bias', 'base_model.gpt_neox.layers.1.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.1.attention.query_key_value.bias', 'base_model.gpt_neox.layers.6.attention.query_key_value.weight', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.3.input_layernorm.weight', 'base_model.gpt_neox.layers.1.attention.query_key_value.weight', 'base_model.gpt_neox.layers.15.attention.dense.weight', 'base_model.gpt_neox.layers.9.input_layernorm.bias', 'base_model.gpt_neox.layers.14.attention.query_key_value.bias', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.4.attention.query_key_value.bias', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.5.input_layernorm.weight', 'base_model.gpt_neox.layers.7.attention.dense.bias', 'base_model.gpt_neox.layers.7.attention.dense.weight', 'base_model.gpt_neox.layers.1.attention.dense.weight', 'base_model.gpt_neox.layers.14.attention.query_key_value.weight', 'base_model.gpt_neox.layers.6.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.9.attention.query_key_value.weight', 'base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.4.input_layernorm.weight', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.2.input_layernorm.bias', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.12.attention.query_key_value.weight', 'base_model.gpt_neox.layers.3.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.1.attention.dense.bias', 'base_model.gpt_neox.layers.6.attention.dense.weight', 'base_model.gpt_neox.layers.7.input_layernorm.bias', 'frozen_head.decoder_blocks.0.attention.dense.weight', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.3.input_layernorm.bias', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.0.input_layernorm.bias', 'base_model.gpt_neox.layers.11.attention.dense.weight', 'frozen_head.decoder_blocks.1.input_layernorm.bias', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.5.input_layernorm.bias', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.12.input_layernorm.bias', 'base_model.gpt_neox.layers.2.input_layernorm.bias', 'base_model.gpt_neox.layers.7.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.10.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.14.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.9.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.0.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.13.attention.dense.weight', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.3.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.7.attention.query_key_value.weight']
- This IS expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs and are newly initialized: ['layers.6.attention.query_key_value.bias', 'layers.4.input_layernorm.bias', 'layers.7.mlp.dense_4h_to_h.bias', 'layers.2.attention.dense.weight', 'layers.4.mlp.dense_h_to_4h.weight', 'layers.10.input_layernorm.bias', 'layers.12.input_layernorm.bias', 'layers.0.attention.query_key_value.bias', 'layers.3.attention.query_key_value.weight', 'layers.4.attention.dense.bias', 'layers.7.mlp.dense_h_to_4h.bias', 'layers.8.mlp.dense_h_to_4h.bias', 'layers.8.attention.dense.bias', 'layers.0.post_attention_layernorm.bias', 'layers.11.input_layernorm.bias', 'layers.7.input_layernorm.bias', 'layers.10.mlp.dense_h_to_4h.bias', 'layers.9.attention.query_key_value.weight', 'final_layer_norm.bias', 'layers.11.attention.dense.bias', 'layers.8.input_layernorm.bias', 'layers.4.input_layernorm.weight', 'layers.4.attention.dense.weight', 'layers.4.post_attention_layernorm.bias', 'layers.4.attention.query_key_value.bias', 'layers.0.input_layernorm.bias', 'layers.1.attention.query_key_value.bias', 'layers.5.mlp.dense_4h_to_h.weight', 'layers.5.attention.query_key_value.weight', 'layers.5.attention.query_key_value.bias', 'layers.12.attention.query_key_value.bias', 'layers.2.mlp.dense_h_to_4h.bias', 'layers.8.post_attention_layernorm.bias', 'layers.0.input_layernorm.weight', 'layers.3.attention.dense.weight', 'layers.4.attention.query_key_value.weight', 'layers.14.mlp.dense_h_to_4h.weight', 'layers.6.mlp.dense_4h_to_h.bias', 'layers.13.attention.query_key_value.weight', 'layers.7.attention.query_key_value.weight', 'layers.5.post_attention_layernorm.weight', 'layers.9.mlp.dense_h_to_4h.weight', 'layers.15.mlp.dense_4h_to_h.weight', 'layers.0.attention.dense.weight', 'layers.12.post_attention_layernorm.bias', 'layers.7.attention.dense.bias', 'layers.13.post_attention_layernorm.bias', 'layers.0.mlp.dense_4h_to_h.bias', 'layers.3.attention.query_key_value.bias', 'layers.12.post_attention_layernorm.weight', 'layers.12.mlp.dense_4h_to_h.bias', 'layers.5.input_layernorm.weight', 'layers.12.mlp.dense_h_to_4h.bias', 'layers.14.input_layernorm.weight', 'layers.0.mlp.dense_h_to_4h.weight', 'layers.2.attention.dense.bias', 'layers.8.attention.query_key_value.weight', 'layers.15.mlp.dense_4h_to_h.bias', 'layers.11.post_attention_layernorm.weight', 'layers.13.input_layernorm.bias', 'layers.12.mlp.dense_h_to_4h.weight', 'layers.13.attention.query_key_value.bias', 'layers.0.attention.dense.bias', 'layers.1.mlp.dense_h_to_4h.weight', 'layers.1.attention.query_key_value.weight', 'layers.1.mlp.dense_4h_to_h.weight', 'layers.15.mlp.dense_h_to_4h.bias', 'layers.14.attention.dense.bias', 'layers.6.input_layernorm.bias', 'layers.8.mlp.dense_4h_to_h.bias', 'layers.13.attention.dense.weight', 'layers.3.input_layernorm.weight', 'layers.2.attention.query_key_value.weight', 'layers.11.attention.dense.weight', 'layers.6.attention.dense.weight', 'layers.3.mlp.dense_4h_to_h.weight', 'layers.1.mlp.dense_4h_to_h.bias', 'layers.3.input_layernorm.bias', 'layers.14.post_attention_layernorm.weight', 'layers.1.post_attention_layernorm.bias', 'layers.15.post_attention_layernorm.weight', 'layers.15.input_layernorm.bias', 'layers.8.mlp.dense_4h_to_h.weight', 'layers.15.attention.dense.weight', 'layers.8.input_layernorm.weight', 'layers.11.mlp.dense_4h_to_h.weight', 'layers.6.attention.dense.bias', 'layers.11.mlp.dense_4h_to_h.bias', 'layers.0.mlp.dense_h_to_4h.bias', 'layers.7.mlp.dense_h_to_4h.weight', 'embed_out.weight', 'layers.4.mlp.dense_h_to_4h.bias', 'layers.10.input_layernorm.weight', 'layers.6.input_layernorm.weight', 'layers.4.mlp.dense_4h_to_h.bias', 'layers.9.mlp.dense_h_to_4h.bias', 'layers.3.post_attention_layernorm.weight', 'layers.10.post_attention_layernorm.weight', 'layers.14.mlp.dense_h_to_4h.bias', 'embed_in.weight', 'layers.11.mlp.dense_h_to_4h.bias', 'layers.1.input_layernorm.bias', 'layers.6.mlp.dense_h_to_4h.bias', 'layers.7.attention.dense.weight', 'layers.6.attention.query_key_value.weight', 'layers.8.attention.query_key_value.bias', 'layers.13.mlp.dense_h_to_4h.weight', 'layers.12.attention.dense.weight', 'layers.3.attention.dense.bias', 'layers.10.mlp.dense_h_to_4h.weight', 'layers.5.mlp.dense_h_to_4h.weight', 'layers.13.mlp.dense_h_to_4h.bias', 'layers.14.attention.query_key_value.weight', 'layers.6.mlp.dense_4h_to_h.weight', 'layers.12.attention.query_key_value.weight', 'layers.5.mlp.dense_4h_to_h.bias', 'layers.15.mlp.dense_h_to_4h.weight', 'layers.15.attention.dense.bias', 'layers.7.mlp.dense_4h_to_h.weight', 'layers.3.mlp.dense_h_to_4h.weight', 'layers.2.mlp.dense_h_to_4h.weight', 'layers.8.mlp.dense_h_to_4h.weight', 'layers.9.attention.query_key_value.bias', 'layers.9.input_layernorm.weight', 'layers.12.input_layernorm.weight', 'layers.12.attention.dense.bias', 'layers.13.post_attention_layernorm.weight', 'layers.7.input_layernorm.weight', 'layers.5.attention.dense.weight', 'layers.0.post_attention_layernorm.weight', 'layers.11.attention.query_key_value.bias', 'layers.3.mlp.dense_h_to_4h.bias', 'layers.5.post_attention_layernorm.bias', 'layers.15.post_attention_layernorm.bias', 'layers.14.attention.query_key_value.bias', 'layers.6.post_attention_layernorm.bias', 'layers.13.mlp.dense_4h_to_h.bias', 'layers.1.input_layernorm.weight', 'layers.9.post_attention_layernorm.bias', 'layers.1.mlp.dense_h_to_4h.bias', 'layers.8.post_attention_layernorm.weight', 'layers.7.post_attention_layernorm.bias', 'layers.2.mlp.dense_4h_to_h.bias', 'layers.13.input_layernorm.weight', 'layers.15.attention.query_key_value.bias', 'layers.9.attention.dense.bias', 'layers.9.attention.dense.weight', 'layers.9.mlp.dense_4h_to_h.weight', 'layers.10.mlp.dense_4h_to_h.bias', 'layers.11.input_layernorm.weight', 'layers.2.mlp.dense_4h_to_h.weight', 'layers.2.input_layernorm.bias', 'layers.2.post_attention_layernorm.weight', 'layers.2.attention.query_key_value.bias', 'layers.14.mlp.dense_4h_to_h.bias', 'layers.7.post_attention_layernorm.weight', 'layers.5.attention.dense.bias', 'layers.10.post_attention_layernorm.bias', 'layers.8.attention.dense.weight', 'layers.10.attention.dense.weight', 'layers.5.input_layernorm.bias', 'layers.6.post_attention_layernorm.weight', 'layers.11.post_attention_layernorm.bias', 'layers.0.attention.query_key_value.weight', 'layers.10.attention.dense.bias', 'layers.11.mlp.dense_h_to_4h.weight', 'layers.12.mlp.dense_4h_to_h.weight', 'layers.10.mlp.dense_4h_to_h.weight', 'layers.14.attention.dense.weight', 'layers.10.attention.query_key_value.weight', 'final_layer_norm.weight', 'layers.4.mlp.dense_4h_to_h.weight', 'layers.4.post_attention_layernorm.weight', 'layers.15.input_layernorm.weight', 'layers.9.input_layernorm.bias', 'layers.14.post_attention_layernorm.bias', 'layers.1.attention.dense.bias', 'layers.14.mlp.dense_4h_to_h.weight', 'layers.15.attention.query_key_value.weight', 'layers.0.mlp.dense_4h_to_h.weight', 'layers.1.attention.dense.weight', 'layers.2.post_attention_layernorm.bias', 'layers.1.post_attention_layernorm.weight', 'layers.6.mlp.dense_h_to_4h.weight', 'layers.11.attention.query_key_value.weight', 'layers.13.mlp.dense_4h_to_h.weight', 'layers.5.mlp.dense_h_to_4h.bias', 'layers.9.mlp.dense_4h_to_h.bias', 'layers.3.mlp.dense_4h_to_h.bias', 'layers.9.post_attention_layernorm.weight', 'layers.13.attention.dense.bias', 'layers.7.attention.query_key_value.bias', 'layers.2.input_layernorm.weight', 'layers.14.input_layernorm.bias', 'layers.10.attention.query_key_value.bias', 'layers.3.post_attention_layernorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs were not used when initializing GPTNeoXForCausalLM: ['base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.3.attention.query_key_value.bias', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.12.input_layernorm.weight', 'base_model.gpt_neox.layers.8.attention.query_key_value.weight', 'base_model.gpt_neox.layers.11.input_layernorm.bias', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.2.attention.dense.bias', 'base_model.gpt_neox.layers.1.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.1.attention.query_key_value.bias', 'frozen_head.decoder_blocks.2.attention.query_key_value.bias', 'v_head.0.bias', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.0.input_layernorm.weight', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.6.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.0.input_layernorm.weight', 'base_model.gpt_neox.layers.12.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.1.input_layernorm.weight', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.1.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.10.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.10.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.12.attention.dense.bias', 'base_model.gpt_neox.layers.15.attention.dense.bias', 'base_model.gpt_neox.layers.4.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.2.attention.dense.weight', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.5.attention.query_key_value.bias', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.attention.query_key_value.weight', 'frozen_head.final_norm.bias', 'frozen_head.decoder_blocks.1.attention.query_key_value.weight', 'base_model.gpt_neox.layers.15.input_layernorm.weight', 'base_model.gpt_neox.layers.7.attention.query_key_value.weight', 'base_model.gpt_neox.layers.11.attention.dense.bias', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.0.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.10.attention.dense.weight', 'base_model.gpt_neox.layers.13.attention.dense.bias', 'base_model.gpt_neox.layers.0.attention.dense.bias', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.4.attention.dense.bias', 'frozen_head.decoder_blocks.2.input_layernorm.bias', 'base_model.gpt_neox.layers.7.attention.dense.bias', 'base_model.gpt_neox.final_layer_norm.bias', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.1.input_layernorm.bias', 'base_model.gpt_neox.layers.2.attention.dense.weight', 'base_model.gpt_neox.layers.15.input_layernorm.bias', 'base_model.gpt_neox.layers.4.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.8.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.13.input_layernorm.bias', 'frozen_head.decoder_blocks.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.13.input_layernorm.weight', 'base_model.gpt_neox.layers.3.input_layernorm.weight', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.6.attention.query_key_value.bias', 'base_model.gpt_neox.layers.12.attention.dense.weight', 'base_model.gpt_neox.layers.15.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.8.attention.query_key_value.bias', 'base_model.gpt_neox.layers.4.input_layernorm.bias', 'v_head.0.weight', 'base_model.gpt_neox.layers.11.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.0.attention.query_key_value.bias', 'frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.1.attention.query_key_value.weight', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.1.attention.query_key_value.bias', 'base_model.gpt_neox.embed_in.weight', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.4.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.2.input_layernorm.weight', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.0.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.13.attention.query_key_value.weight', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.2.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.14.attention.query_key_value.bias', 'base_model.gpt_neox.layers.3.attention.query_key_value.bias', 'frozen_head.decoder_blocks.0.attention.query_key_value.bias', 'base_model.gpt_neox.layers.14.attention.query_key_value.weight', 'base_model.gpt_neox.layers.5.attention.dense.weight', 'base_model.gpt_neox.layers.5.input_layernorm.bias', 'base_model.gpt_neox.layers.10.attention.query_key_value.bias', 'base_model.gpt_neox.layers.3.input_layernorm.bias', 'base_model.gpt_neox.layers.5.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.15.attention.query_key_value.bias', 'base_model.gpt_neox.layers.6.input_layernorm.bias', 'base_model.gpt_neox.layers.14.input_layernorm.weight', 'base_model.gpt_neox.layers.2.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.6.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.0.input_layernorm.bias', 'frozen_head.decoder_blocks.3.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.9.attention.dense.weight', 'frozen_head.decoder_blocks.3.attention.dense.weight', 'base_model.gpt_neox.layers.0.attention.query_key_value.weight', 'base_model.gpt_neox.layers.9.attention.query_key_value.weight', 'v_head.2.weight', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.8.attention.dense.weight', 'frozen_head.decoder_blocks.2.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.12.attention.query_key_value.weight', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.15.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.3.attention.dense.bias', 'base_model.gpt_neox.layers.13.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.10.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.4.attention.query_key_value.bias', 'base_model.gpt_neox.layers.5.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.5.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.8.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.0.attention.dense.bias', 'base_model.gpt_neox.layers.1.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.7.input_layernorm.weight', 'base_model.gpt_neox.layers.7.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.1.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.9.input_layernorm.weight', 'frozen_head.decoder_blocks.2.attention.query_key_value.weight', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.2.attention.query_key_value.weight', 'base_model.gpt_neox.layers.15.attention.query_key_value.weight', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.13.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.4.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.weight', 'frozen_head.decoder_blocks.3.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.10.attention.dense.bias', 'base_model.gpt_neox.layers.11.attention.query_key_value.weight', 'base_model.gpt_neox.layers.9.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.0.input_layernorm.bias', 'base_model.gpt_neox.layers.2.attention.query_key_value.bias', 'base_model.gpt_neox.layers.5.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.2.input_layernorm.bias', 'base_model.gpt_neox.layers.0.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.6.attention.dense.bias', 'base_model.gpt_neox.layers.10.attention.query_key_value.weight', 'frozen_head.decoder_blocks.0.attention.query_key_value.weight', 'base_model.gpt_neox.layers.12.input_layernorm.bias', 'base_model.gpt_neox.layers.9.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.0.attention.dense.weight', 'base_model.gpt_neox.layers.8.attention.dense.bias', 'base_model.gpt_neox.layers.15.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.0.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.1.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.2.input_layernorm.weight', 'base_model.gpt_neox.layers.7.attention.dense.weight', 'base_model.gpt_neox.layers.0.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.6.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.6.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.8.input_layernorm.bias', 'base_model.gpt_neox.layers.9.input_layernorm.bias', 'base_model.gpt_neox.layers.14.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.7.input_layernorm.bias', 'frozen_head.decoder_blocks.0.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.12.attention.query_key_value.bias', 'base_model.gpt_neox.layers.11.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.12.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.7.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.9.attention.dense.bias', 'base_model.gpt_neox.layers.4.attention.query_key_value.weight', 'base_model.gpt_neox.layers.7.attention.query_key_value.bias', 'frozen_head.decoder_blocks.2.attention.dense.bias', 'base_model.gpt_neox.layers.1.mlp.dense_4h_to_h.weight', 'v_head.2.bias', 'base_model.gpt_neox.layers.11.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.12.mlp.dense_4h_to_h.bias', 'frozen_head.decoder_blocks.1.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.12.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.11.input_layernorm.weight', 'frozen_head.decoder_blocks.3.mlp.dense_h_to_4h.bias', 'frozen_head.decoder_blocks.1.input_layernorm.bias', 'base_model.gpt_neox.layers.13.attention.query_key_value.bias', 'base_model.gpt_neox.layers.15.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.1.attention.dense.weight', 'base_model.gpt_neox.layers.10.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.3.attention.dense.weight', 'base_model.gpt_neox.layers.13.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.0.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.7.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.9.attention.query_key_value.bias', 'frozen_head.decoder_blocks.1.attention.dense.bias', 'base_model.gpt_neox.layers.2.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.8.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.8.input_layernorm.weight', 'frozen_head.decoder_blocks.1.attention.dense.weight', 'base_model.gpt_neox.layers.11.attention.query_key_value.bias', 'base_model.embed_out.weight', 'base_model.gpt_neox.layers.2.mlp.dense_h_to_4h.weight', 'base_model.gpt_neox.layers.3.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.9.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.7.mlp.dense_4h_to_h.bias', 'base_model.gpt_neox.layers.2.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.0.attention.dense.weight', 'base_model.gpt_neox.layers.13.attention.dense.weight', 'base_model.gpt_neox.layers.1.attention.dense.bias', 'base_model.gpt_neox.layers.3.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.4.input_layernorm.weight', 'base_model.gpt_neox.layers.11.attention.dense.weight', 'base_model.gpt_neox.layers.1.input_layernorm.weight', 'base_model.gpt_neox.layers.3.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.6.attention.dense.weight', 'base_model.gpt_neox.layers.6.attention.query_key_value.weight', 'base_model.gpt_neox.layers.6.input_layernorm.weight', 'base_model.gpt_neox.layers.15.attention.dense.weight', 'frozen_head.decoder_blocks.3.input_layernorm.bias', 'base_model.gpt_neox.layers.13.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.3.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.14.post_attention_layernorm.weight', 'base_model.gpt_neox.layers.8.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.4.attention.dense.weight', 'frozen_head.decoder_blocks.1.post_attention_layernorm.bias', 'base_model.gpt_neox.layers.10.input_layernorm.weight', 'frozen_head.decoder_blocks.3.attention.query_key_value.weight', 'frozen_head.lm_head.weight', 'base_model.gpt_neox.layers.3.attention.dense.bias', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.14.mlp.dense_4h_to_h.weight', 'base_model.gpt_neox.layers.14.attention.dense.weight', 'base_model.gpt_neox.layers.5.input_layernorm.weight', 'base_model.gpt_neox.layers.14.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.final_layer_norm.weight', 'base_model.gpt_neox.layers.11.mlp.dense_h_to_4h.bias', 'base_model.gpt_neox.layers.9.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.2.post_attention_layernorm.bias', 'frozen_head.decoder_blocks.0.mlp.dense_h_to_4h.weight', 'frozen_head.decoder_blocks.2.post_attention_layernorm.weight', 'frozen_head.decoder_blocks.3.input_layernorm.weight', 'base_model.gpt_neox.layers.5.attention.dense.bias', 'base_model.gpt_neox.layers.14.input_layernorm.bias', 'base_model.gpt_neox.layers.10.input_layernorm.bias', 'frozen_head.final_norm.weight', 'base_model.gpt_neox.layers.14.attention.dense.bias', 'base_model.gpt_neox.layers.5.attention.query_key_value.weight']
- This IS expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at lomahony/pythia-1b-helpful-sfted1-ppo-3epochs and are newly initialized: ['layers.10.mlp.dense_4h_to_h.bias', 'layers.11.post_attention_layernorm.weight', 'layers.10.attention.dense.bias', 'layers.5.attention.query_key_value.weight', 'layers.6.mlp.dense_h_to_4h.weight', 'layers.3.attention.dense.weight', 'layers.6.mlp.dense_h_to_4h.bias', 'layers.15.mlp.dense_4h_to_h.bias', 'layers.13.attention.query_key_value.weight', 'layers.4.post_attention_layernorm.weight', 'layers.2.post_attention_layernorm.weight', 'layers.12.mlp.dense_h_to_4h.weight', 'layers.0.attention.query_key_value.weight', 'layers.11.mlp.dense_4h_to_h.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.dense_4h_to_h.bias', 'layers.10.input_layernorm.weight', 'layers.9.mlp.dense_4h_to_h.weight', 'layers.12.attention.dense.weight', 'layers.14.attention.dense.weight', 'layers.6.attention.dense.bias', 'layers.3.attention.query_key_value.weight', 'layers.2.mlp.dense_h_to_4h.bias', 'layers.6.attention.query_key_value.bias', 'layers.4.attention.dense.weight', 'layers.5.post_attention_layernorm.weight', 'layers.9.post_attention_layernorm.bias', 'layers.4.attention.dense.bias', 'layers.4.attention.query_key_value.weight', 'layers.6.mlp.dense_4h_to_h.bias', 'layers.15.input_layernorm.weight', 'layers.8.post_attention_layernorm.weight', 'layers.9.attention.dense.bias', 'layers.0.mlp.dense_h_to_4h.bias', 'layers.7.post_attention_layernorm.bias', 'layers.5.post_attention_layernorm.bias', 'layers.6.post_attention_layernorm.weight', 'layers.15.mlp.dense_h_to_4h.bias', 'embed_out.weight', 'layers.10.attention.dense.weight', 'layers.11.attention.query_key_value.bias', 'layers.3.mlp.dense_h_to_4h.bias', 'layers.11.attention.dense.bias', 'layers.10.mlp.dense_h_to_4h.weight', 'layers.6.attention.query_key_value.weight', 'layers.10.attention.query_key_value.weight', 'layers.10.post_attention_layernorm.weight', 'layers.8.attention.query_key_value.bias', 'layers.5.attention.dense.weight', 'layers.9.mlp.dense_4h_to_h.bias', 'layers.10.mlp.dense_h_to_4h.bias', 'layers.12.attention.query_key_value.weight', 'layers.12.mlp.dense_4h_to_h.weight', 'layers.8.attention.dense.bias', 'layers.13.mlp.dense_4h_to_h.weight', 'layers.3.post_attention_layernorm.weight', 'layers.7.post_attention_layernorm.weight', 'layers.3.attention.query_key_value.bias', 'final_layer_norm.weight', 'layers.14.attention.query_key_value.weight', 'layers.1.input_layernorm.weight', 'layers.2.input_layernorm.bias', 'layers.9.input_layernorm.bias', 'layers.5.attention.query_key_value.bias', 'layers.15.mlp.dense_h_to_4h.weight', 'layers.11.mlp.dense_h_to_4h.weight', 'layers.2.attention.query_key_value.bias', 'layers.15.attention.dense.weight', 'layers.12.post_attention_layernorm.weight', 'layers.3.mlp.dense_4h_to_h.bias', 'layers.15.mlp.dense_4h_to_h.weight', 'layers.9.mlp.dense_h_to_4h.bias', 'layers.14.mlp.dense_h_to_4h.bias', 'layers.7.mlp.dense_h_to_4h.bias', 'layers.12.input_layernorm.weight', 'layers.8.input_layernorm.bias', 'layers.1.mlp.dense_h_to_4h.bias', 'layers.0.input_layernorm.weight', 'layers.8.attention.dense.weight', 'layers.4.mlp.dense_h_to_4h.bias', 'layers.14.post_attention_layernorm.bias', 'layers.3.mlp.dense_4h_to_h.weight', 'layers.2.mlp.dense_4h_to_h.bias', 'layers.0.attention.query_key_value.bias', 'layers.5.mlp.dense_4h_to_h.weight', 'layers.0.attention.dense.weight', 'layers.8.mlp.dense_4h_to_h.weight', 'layers.11.post_attention_layernorm.bias', 'layers.1.attention.dense.bias', 'layers.5.input_layernorm.weight', 'layers.12.post_attention_layernorm.bias', 'layers.15.attention.dense.bias', 'layers.11.mlp.dense_h_to_4h.bias', 'layers.12.input_layernorm.bias', 'layers.9.attention.query_key_value.bias', 'layers.1.mlp.dense_4h_to_h.bias', 'layers.7.mlp.dense_h_to_4h.weight', 'layers.13.mlp.dense_h_to_4h.weight', 'layers.0.mlp.dense_4h_to_h.weight', 'layers.1.input_layernorm.bias', 'embed_in.weight', 'layers.3.input_layernorm.weight', 'layers.5.attention.dense.bias', 'layers.0.post_attention_layernorm.weight', 'layers.13.post_attention_layernorm.bias', 'layers.1.post_attention_layernorm.bias', 'layers.2.mlp.dense_h_to_4h.weight', 'layers.12.attention.query_key_value.bias', 'layers.14.mlp.dense_4h_to_h.bias', 'layers.1.attention.query_key_value.bias', 'layers.2.input_layernorm.weight', 'layers.5.mlp.dense_4h_to_h.bias', 'layers.7.input_layernorm.bias', 'layers.1.attention.dense.weight', 'layers.12.mlp.dense_4h_to_h.bias', 'layers.13.input_layernorm.bias', 'layers.11.mlp.dense_4h_to_h.bias', 'layers.15.attention.query_key_value.weight', 'layers.6.input_layernorm.bias', 'layers.8.mlp.dense_h_to_4h.weight', 'layers.14.input_layernorm.bias', 'layers.0.post_attention_layernorm.bias', 'layers.14.post_attention_layernorm.weight', 'layers.14.mlp.dense_4h_to_h.weight', 'layers.7.mlp.dense_4h_to_h.weight', 'layers.5.mlp.dense_h_to_4h.weight', 'layers.11.attention.dense.weight', 'layers.7.attention.query_key_value.weight', 'layers.15.attention.query_key_value.bias', 'layers.14.mlp.dense_h_to_4h.weight', 'layers.1.mlp.dense_h_to_4h.weight', 'layers.8.input_layernorm.weight', 'layers.15.input_layernorm.bias', 'layers.6.attention.dense.weight', 'layers.7.attention.dense.bias', 'layers.10.post_attention_layernorm.bias', 'layers.12.attention.dense.bias', 'layers.4.input_layernorm.bias', 'layers.7.attention.dense.weight', 'layers.5.input_layernorm.bias', 'layers.15.post_attention_layernorm.weight', 'layers.4.mlp.dense_4h_to_h.weight', 'layers.3.mlp.dense_h_to_4h.weight', 'layers.3.attention.dense.bias', 'layers.9.post_attention_layernorm.weight', 'layers.13.mlp.dense_h_to_4h.bias', 'layers.7.mlp.dense_4h_to_h.bias', 'layers.11.input_layernorm.weight', 'layers.5.mlp.dense_h_to_4h.bias', 'layers.6.input_layernorm.weight', 'layers.13.attention.dense.bias', 'layers.4.post_attention_layernorm.bias', 'layers.12.mlp.dense_h_to_4h.bias', 'layers.4.mlp.dense_h_to_4h.weight', 'layers.7.attention.query_key_value.bias', 'layers.13.post_attention_layernorm.weight', 'layers.0.mlp.dense_h_to_4h.weight', 'layers.13.attention.dense.weight', 'layers.2.attention.dense.weight', 'layers.8.post_attention_layernorm.bias', 'layers.11.input_layernorm.bias', 'layers.11.attention.query_key_value.weight', 'layers.1.attention.query_key_value.weight', 'layers.8.mlp.dense_h_to_4h.bias', 'layers.14.attention.query_key_value.bias', 'layers.0.attention.dense.bias', 'layers.8.mlp.dense_4h_to_h.bias', 'layers.14.input_layernorm.weight', 'layers.2.attention.dense.bias', 'layers.10.input_layernorm.bias', 'layers.4.attention.query_key_value.bias', 'layers.6.post_attention_layernorm.bias', 'layers.6.mlp.dense_4h_to_h.weight', 'layers.4.input_layernorm.weight', 'layers.7.input_layernorm.weight', 'layers.14.attention.dense.bias', 'layers.2.attention.query_key_value.weight', 'layers.0.input_layernorm.bias', 'layers.9.attention.query_key_value.weight', 'layers.1.mlp.dense_4h_to_h.weight', 'layers.4.mlp.dense_4h_to_h.bias', 'layers.2.mlp.dense_4h_to_h.weight', 'layers.8.attention.query_key_value.weight', 'layers.10.attention.query_key_value.bias', 'layers.9.input_layernorm.weight', 'layers.10.mlp.dense_4h_to_h.weight', 'layers.13.attention.query_key_value.bias', 'layers.3.post_attention_layernorm.bias', 'layers.9.attention.dense.weight', 'layers.2.post_attention_layernorm.bias', 'layers.15.post_attention_layernorm.bias', 'layers.1.post_attention_layernorm.weight', 'layers.9.mlp.dense_h_to_4h.weight', 'layers.0.mlp.dense_4h_to_h.bias', 'layers.3.input_layernorm.bias', 'final_layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-03-13:16:37:27,357 INFO     [huggingface.py:298] Using 8 devices with data parallelism
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-03-13:16:37:38,535 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-03-13:16:37:38,535 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
2024-03-13:16:37:38,727 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-03-13:16:37:38,727 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
2024-03-13:16:37:39,283 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-03-13:16:37:39,283 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
2024-03-13:16:37:39,491 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-03-13:16:37:39,491 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
2024-03-13:16:37:39,925 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-03-13:16:37:39,925 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
2024-03-13:16:37:40,249 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-03-13:16:37:40,249 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
2024-03-13:16:37:40,752 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-03-13:16:37:40,752 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
2024-03-13:16:37:42,063 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-03-13:16:37:42,063 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-03-13:16:37:48,465 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:37:48,465 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:37:48,502 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:37:48,502 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:37:48,615 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:37:48,615 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:37:49,098 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:37:49,098 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:37:49,130 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:37:49,130 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:37:49,955 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:37:49,956 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:37:49,962 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:37:49,962 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:37:51,190 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:37:51,190 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-03-13:16:38:05,459 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:05,460 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:05,460 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:05,460 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:05,460 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-03-13:16:38:05,460 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:05,629 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:05,630 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:05,630 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:05,630 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:05,630 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-03-13:16:38:05,630 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:05,904 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:05,904 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:05,904 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:05,904 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:05,904 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-03-13:16:38:05,904 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:06,183 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:06,183 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:06,183 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:06,183 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:06,183 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-03-13:16:38:06,183 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:06,441 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:06,442 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:06,442 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:06,442 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:06,442 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-03-13:16:38:06,442 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:07,248 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:07,248 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:07,248 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:07,248 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:07,248 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-03-13:16:38:07,248 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:08,395 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:08,395 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:08,395 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:08,395 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:08,395 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-03-13:16:38:08,395 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:08,985 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:08,985 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:08,985 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-03-13:16:38:08,985 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:08,985 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-03-13:16:38:08,985 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-03-13:16:38:10,155 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-03-13:16:38:10,155 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-03-13:16:38:10,155 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-03-13:16:38:10,155 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-03-13:16:38:10,155 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-03-13:16:38:10,155 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-03-13:16:38:10,155 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-03-13:16:38:10,155 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-03-13:16:38:10,155 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-03-13:16:38:10,155 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-03-13:16:38:10,156 INFO     [task.py:337] Building contexts for task on rank 7...
2024-03-13:16:38:10,228 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-03-13:16:38:10,228 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-03-13:16:38:10,228 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-03-13:16:38:10,228 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-03-13:16:38:10,228 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-03-13:16:38:10,228 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-03-13:16:38:10,229 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-03-13:16:38:10,229 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-03-13:16:38:10,229 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-03-13:16:38:10,229 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-03-13:16:38:10,229 INFO     [task.py:337] Building contexts for task on rank 6...
2024-03-13:16:38:10,865 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-03-13:16:38:10,865 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-03-13:16:38:10,865 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-03-13:16:38:10,865 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-03-13:16:38:10,866 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-03-13:16:38:10,866 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-03-13:16:38:10,866 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-03-13:16:38:10,866 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-03-13:16:38:10,866 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-03-13:16:38:10,866 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-03-13:16:38:10,866 INFO     [task.py:337] Building contexts for task on rank 1...
2024-03-13:16:38:11,094 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-03-13:16:38:11,094 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-03-13:16:38:11,094 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-03-13:16:38:11,094 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-03-13:16:38:11,094 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-03-13:16:38:11,094 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-03-13:16:38:11,094 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-03-13:16:38:11,094 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-03-13:16:38:11,094 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-03-13:16:38:11,094 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-03-13:16:38:11,094 INFO     [task.py:337] Building contexts for task on rank 4...
2024-03-13:16:38:11,281 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-03-13:16:38:11,281 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-03-13:16:38:11,281 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-03-13:16:38:11,281 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-03-13:16:38:11,281 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-03-13:16:38:11,281 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-03-13:16:38:11,281 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-03-13:16:38:11,281 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-03-13:16:38:11,281 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-03-13:16:38:11,281 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-03-13:16:38:11,282 INFO     [task.py:337] Building contexts for task on rank 2...
2024-03-13:16:38:12,059 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-03-13:16:38:12,059 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-03-13:16:38:12,059 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-03-13:16:38:12,059 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-03-13:16:38:12,059 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-03-13:16:38:12,059 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-03-13:16:38:12,059 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-03-13:16:38:12,059 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-03-13:16:38:12,059 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-03-13:16:38:12,059 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-03-13:16:38:12,059 INFO     [task.py:337] Building contexts for task on rank 0...
2024-03-13:16:38:13,256 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-03-13:16:38:13,256 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-03-13:16:38:13,256 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-03-13:16:38:13,256 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-03-13:16:38:13,256 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-03-13:16:38:13,256 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-03-13:16:38:13,256 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-03-13:16:38:13,256 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-03-13:16:38:13,256 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-03-13:16:38:13,256 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-03-13:16:38:13,256 INFO     [task.py:337] Building contexts for task on rank 5...
2024-03-13:16:38:13,596 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-03-13:16:38:13,596 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-03-13:16:38:13,596 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-03-13:16:38:13,596 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-03-13:16:38:13,596 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-03-13:16:38:13,596 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-03-13:16:38:13,596 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-03-13:16:38:13,596 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-03-13:16:38:13,596 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-03-13:16:38:13,596 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-03-13:16:38:13,596 INFO     [task.py:337] Building contexts for task on rank 3...
2024-03-13:16:38:25,301 INFO     [task.py:337] Building contexts for task on rank 3...
2024-03-13:16:38:25,301 INFO     [task.py:337] Building contexts for task on rank 1...
2024-03-13:16:38:25,302 INFO     [task.py:337] Building contexts for task on rank 4...
2024-03-13:16:38:25,302 INFO     [task.py:337] Building contexts for task on rank 6...
2024-03-13:16:38:25,302 INFO     [task.py:337] Building contexts for task on rank 5...
2024-03-13:16:38:25,302 INFO     [task.py:337] Building contexts for task on rank 0...
2024-03-13:16:38:25,302 INFO     [task.py:337] Building contexts for task on rank 7...
2024-03-13:16:38:25,302 INFO     [task.py:337] Building contexts for task on rank 2...
2024-03-13:16:38:25,732 INFO     [task.py:337] Building contexts for task on rank 3...
2024-03-13:16:38:25,732 INFO     [task.py:337] Building contexts for task on rank 1...
2024-03-13:16:38:25,732 INFO     [task.py:337] Building contexts for task on rank 5...
2024-03-13:16:38:25,732 INFO     [task.py:337] Building contexts for task on rank 2...
2024-03-13:16:38:25,732 INFO     [task.py:337] Building contexts for task on rank 6...
2024-03-13:16:38:25,732 INFO     [task.py:337] Building contexts for task on rank 0...
2024-03-13:16:38:25,732 INFO     [task.py:337] Building contexts for task on rank 4...
2024-03-13:16:38:25,732 INFO     [task.py:337] Building contexts for task on rank 7...
2024-03-13:16:38:26,076 INFO     [task.py:337] Building contexts for task on rank 1...
2024-03-13:16:38:26,076 INFO     [task.py:337] Building contexts for task on rank 3...
2024-03-13:16:38:26,076 INFO     [task.py:337] Building contexts for task on rank 0...
2024-03-13:16:38:26,076 INFO     [task.py:337] Building contexts for task on rank 6...
2024-03-13:16:38:26,076 INFO     [task.py:337] Building contexts for task on rank 4...
2024-03-13:16:38:26,076 INFO     [task.py:337] Building contexts for task on rank 7...
2024-03-13:16:38:26,076 INFO     [task.py:337] Building contexts for task on rank 2...
2024-03-13:16:38:26,076 INFO     [task.py:337] Building contexts for task on rank 5...
2024-03-13:16:38:27,531 INFO     [task.py:337] Building contexts for task on rank 6...
2024-03-13:16:38:27,531 INFO     [task.py:337] Building contexts for task on rank 5...
2024-03-13:16:38:27,531 INFO     [task.py:337] Building contexts for task on rank 4...
2024-03-13:16:38:27,531 INFO     [task.py:337] Building contexts for task on rank 3...
2024-03-13:16:38:27,531 INFO     [task.py:337] Building contexts for task on rank 1...
2024-03-13:16:38:27,531 INFO     [task.py:337] Building contexts for task on rank 7...
2024-03-13:16:38:27,531 INFO     [task.py:337] Building contexts for task on rank 0...
2024-03-13:16:38:27,531 INFO     [task.py:337] Building contexts for task on rank 2...
2024-03-13:16:38:28,663 INFO     [task.py:337] Building contexts for task on rank 1...
2024-03-13:16:38:28,663 INFO     [task.py:337] Building contexts for task on rank 6...
2024-03-13:16:38:28,663 INFO     [task.py:337] Building contexts for task on rank 7...
2024-03-13:16:38:28,663 INFO     [task.py:337] Building contexts for task on rank 3...
2024-03-13:16:38:28,663 INFO     [task.py:337] Building contexts for task on rank 5...
2024-03-13:16:38:28,663 INFO     [task.py:337] Building contexts for task on rank 0...
2024-03-13:16:38:28,663 INFO     [task.py:337] Building contexts for task on rank 4...
2024-03-13:16:38:28,663 INFO     [task.py:337] Building contexts for task on rank 2...
2024-03-13:16:38:28,729 INFO     [task.py:337] Building contexts for task on rank 7...
2024-03-13:16:38:28,729 INFO     [task.py:337] Building contexts for task on rank 6...
2024-03-13:16:38:28,729 INFO     [task.py:337] Building contexts for task on rank 5...
2024-03-13:16:38:28,729 INFO     [task.py:337] Building contexts for task on rank 3...
2024-03-13:16:38:28,729 INFO     [task.py:337] Building contexts for task on rank 1...
2024-03-13:16:38:28,729 INFO     [task.py:337] Building contexts for task on rank 4...
2024-03-13:16:38:28,729 INFO     [task.py:337] Building contexts for task on rank 0...
2024-03-13:16:38:28,729 INFO     [task.py:337] Building contexts for task on rank 2...
2024-03-13:16:38:29,029 INFO     [task.py:337] Building contexts for task on rank 4...
2024-03-13:16:38:29,029 INFO     [task.py:337] Building contexts for task on rank 6...
2024-03-13:16:38:29,029 INFO     [task.py:337] Building contexts for task on rank 3...
2024-03-13:16:38:29,029 INFO     [task.py:337] Building contexts for task on rank 1...
2024-03-13:16:38:29,029 INFO     [task.py:337] Building contexts for task on rank 5...
2024-03-13:16:38:29,029 INFO     [task.py:337] Building contexts for task on rank 0...
2024-03-13:16:38:29,029 INFO     [task.py:337] Building contexts for task on rank 7...
2024-03-13:16:38:29,029 INFO     [task.py:337] Building contexts for task on rank 2...
2024-03-13:16:38:29,227 INFO     [task.py:337] Building contexts for task on rank 6...
2024-03-13:16:38:29,227 INFO     [task.py:337] Building contexts for task on rank 4...
2024-03-13:16:38:29,227 INFO     [task.py:337] Building contexts for task on rank 1...
2024-03-13:16:38:29,227 INFO     [task.py:337] Building contexts for task on rank 5...
2024-03-13:16:38:29,227 INFO     [task.py:337] Building contexts for task on rank 7...
2024-03-13:16:38:29,227 INFO     [task.py:337] Building contexts for task on rank 2...
2024-03-13:16:38:29,227 INFO     [task.py:337] Building contexts for task on rank 0...
2024-03-13:16:38:29,227 INFO     [task.py:337] Building contexts for task on rank 3...
2024-03-13:16:38:29,261 INFO     [task.py:337] Building contexts for task on rank 3...
2024-03-13:16:38:29,261 INFO     [task.py:337] Building contexts for task on rank 1...
2024-03-13:16:38:29,261 INFO     [task.py:337] Building contexts for task on rank 5...
2024-03-13:16:38:29,261 INFO     [task.py:337] Building contexts for task on rank 7...
2024-03-13:16:38:29,261 INFO     [task.py:337] Building contexts for task on rank 6...
2024-03-13:16:38:29,261 INFO     [task.py:337] Building contexts for task on rank 0...
2024-03-13:16:38:29,261 INFO     [task.py:337] Building contexts for task on rank 4...
2024-03-13:16:38:29,261 INFO     [task.py:337] Building contexts for task on rank 2...
2024-03-13:16:38:29,340 INFO     [evaluator.py:314] Running loglikelihood requests
2024-03-13:16:38:29,340 INFO     [evaluator.py:314] Running loglikelihood requests
2024-03-13:16:38:29,340 INFO     [evaluator.py:314] Running loglikelihood requests
2024-03-13:16:38:29,340 INFO     [evaluator.py:314] Running loglikelihood requests
2024-03-13:16:38:29,340 INFO     [evaluator.py:314] Running loglikelihood requests
2024-03-13:16:38:29,340 INFO     [evaluator.py:314] Running loglikelihood requests
2024-03-13:16:38:29,340 INFO     [evaluator.py:314] Running loglikelihood requests
2024-03-13:16:38:29,340 INFO     [evaluator.py:314] Running loglikelihood requests
  0%|          | 0/9788 [00:00<?, ?it/s]  0%|          | 1/9788 [00:02<5:40:23,  2.09s/it]  0%|          | 33/9788 [00:02<08:06, 20.07it/s]   1%|          | 65/9788 [00:02<03:47, 42.81it/s]  1%|          | 97/9788 [00:02<02:20, 68.76it/s]  1%|▏         | 129/9788 [00:02<01:39, 97.46it/s]  2%|▏         | 161/9788 [00:02<01:14, 128.51it/s]  2%|▏         | 193/9788 [00:02<00:59, 161.10it/s]  2%|▏         | 225/9788 [00:02<00:49, 192.26it/s]  3%|▎         | 273/9788 [00:03<00:41, 232.03it/s]  3%|▎         | 321/9788 [00:03<00:35, 263.71it/s]  4%|▍         | 369/9788 [00:03<00:32, 291.69it/s]  4%|▍         | 417/9788 [00:03<00:29, 314.87it/s]  5%|▍         | 465/9788 [00:03<00:27, 333.34it/s]  5%|▌         | 513/9788 [00:03<00:26, 348.36it/s]  6%|▌         | 561/9788 [00:03<00:25, 360.12it/s]  6%|▌         | 609/9788 [00:03<00:24, 371.21it/s]  7%|▋         | 657/9788 [00:04<00:23, 381.37it/s]  7%|▋         | 705/9788 [00:04<00:23, 389.68it/s]  8%|▊         | 753/9788 [00:04<00:22, 396.10it/s]  8%|▊         | 801/9788 [00:04<00:22, 400.17it/s]  9%|▊         | 849/9788 [00:04<00:22, 405.42it/s]  9%|▉         | 897/9788 [00:04<00:21, 407.43it/s] 10%|▉         | 945/9788 [00:04<00:21, 408.98it/s] 10%|█         | 993/9788 [00:04<00:21, 409.83it/s] 11%|█         | 1041/9788 [00:05<00:21, 410.55it/s] 11%|█         | 1089/9788 [00:05<00:21, 412.54it/s] 12%|█▏        | 1137/9788 [00:05<00:20, 412.51it/s] 12%|█▏        | 1185/9788 [00:05<00:20, 413.24it/s] 13%|█▎        | 1233/9788 [00:05<00:20, 413.88it/s] 13%|█▎        | 1281/9788 [00:05<00:20, 414.76it/s] 14%|█▎        | 1329/9788 [00:05<00:20, 417.72it/s] 14%|█▍        | 1377/9788 [00:05<00:20, 417.95it/s] 15%|█▍        | 1425/9788 [00:05<00:20, 415.20it/s] 15%|█▌        | 1473/9788 [00:06<00:19, 417.17it/s] 16%|█▌        | 1521/9788 [00:06<00:19, 417.77it/s] 16%|█▌        | 1569/9788 [00:06<00:19, 417.76it/s] 17%|█▋        | 1617/9788 [00:06<00:19, 418.04it/s] 17%|█▋        | 1665/9788 [00:06<00:19, 419.83it/s] 18%|█▊        | 1713/9788 [00:06<00:19, 419.90it/s] 18%|█▊        | 1761/9788 [00:06<00:19, 419.36it/s] 18%|█▊        | 1809/9788 [00:06<00:18, 420.32it/s] 19%|█▉        | 1857/9788 [00:06<00:18, 420.74it/s] 19%|█▉        | 1905/9788 [00:07<00:18, 420.22it/s] 20%|█▉        | 1953/9788 [00:07<00:18, 430.31it/s] 20%|██        | 2001/9788 [00:07<00:17, 443.88it/s] 21%|██        | 2049/9788 [00:07<00:17, 453.79it/s] 21%|██▏       | 2097/9788 [00:07<00:16, 458.17it/s] 22%|██▏       | 2145/9788 [00:07<00:16, 460.87it/s] 22%|██▏       | 2193/9788 [00:07<00:16, 464.99it/s] 23%|██▎       | 2241/9788 [00:07<00:16, 467.21it/s] 23%|██▎       | 2289/9788 [00:07<00:16, 468.03it/s] 24%|██▍       | 2337/9788 [00:08<00:15, 469.20it/s] 24%|██▍       | 2385/9788 [00:08<00:15, 471.40it/s] 25%|██▍       | 2433/9788 [00:08<00:15, 471.82it/s] 25%|██▌       | 2481/9788 [00:08<00:15, 472.10it/s] 26%|██▌       | 2529/9788 [00:08<00:15, 472.17it/s] 26%|██▋       | 2577/9788 [00:08<00:15, 474.31it/s] 27%|██▋       | 2625/9788 [00:08<00:15, 475.00it/s] 27%|██▋       | 2673/9788 [00:08<00:14, 474.85it/s] 28%|██▊       | 2721/9788 [00:08<00:14, 473.96it/s] 28%|██▊       | 2776/9788 [00:08<00:14, 496.43it/s] 29%|██▉       | 2830/9788 [00:09<00:13, 509.22it/s] 29%|██▉       | 2881/9788 [00:09<00:14, 461.33it/s] 30%|██▉       | 2929/9788 [00:09<00:14, 466.37it/s] 30%|███       | 2983/9788 [00:09<00:13, 487.23it/s] 31%|███       | 3033/9788 [00:09<00:13, 489.99it/s] 31%|███▏      | 3083/9788 [00:09<00:13, 492.37it/s] 32%|███▏      | 3133/9788 [00:09<00:13, 493.78it/s] 33%|███▎      | 3185/9788 [00:09<00:14, 460.73it/s] 33%|███▎      | 3235/9788 [00:09<00:13, 471.67it/s] 34%|███▎      | 3283/9788 [00:09<00:13, 473.96it/s] 34%|███▍      | 3331/9788 [00:10<00:13, 475.53it/s] 35%|███▍      | 3386/9788 [00:10<00:12, 497.27it/s] 35%|███▌      | 3437/9788 [00:10<00:12, 500.89it/s] 36%|███▌      | 3488/9788 [00:10<00:12, 503.02it/s] 36%|███▌      | 3539/9788 [00:10<00:13, 464.91it/s] 37%|███▋      | 3599/9788 [00:10<00:12, 502.79it/s] 37%|███▋      | 3650/9788 [00:10<00:13, 462.89it/s] 38%|███▊      | 3701/9788 [00:10<00:12, 475.65it/s] 38%|███▊      | 3761/9788 [00:10<00:12, 470.49it/s] 39%|███▉      | 3813/9788 [00:11<00:12, 483.81it/s] 40%|███▉      | 3868/9788 [00:11<00:11, 502.05it/s] 40%|████      | 3921/9788 [00:11<00:12, 471.34it/s] 41%|████      | 3982/9788 [00:11<00:11, 509.09it/s] 41%|████      | 4034/9788 [00:11<00:12, 472.38it/s] 42%|████▏     | 4093/9788 [00:11<00:11, 504.05it/s] 42%|████▏     | 4145/9788 [00:11<00:11, 471.32it/s] 43%|████▎     | 4198/9788 [00:11<00:11, 486.96it/s] 43%|████▎     | 4254/9788 [00:11<00:10, 507.11it/s] 44%|████▍     | 4306/9788 [00:12<00:11, 478.38it/s] 45%|████▍     | 4369/9788 [00:12<00:11, 484.43it/s] 45%|████▌     | 4433/9788 [00:12<00:10, 487.63it/s] 46%|████▌     | 4497/9788 [00:12<00:10, 493.60it/s] 47%|████▋     | 4561/9788 [00:12<00:10, 497.26it/s] 47%|████▋     | 4625/9788 [00:12<00:10, 500.78it/s] 48%|████▊     | 4689/9788 [00:12<00:10, 502.24it/s] 49%|████▊     | 4753/9788 [00:12<00:09, 504.61it/s] 49%|████▉     | 4817/9788 [00:13<00:09, 505.89it/s] 50%|████▉     | 4881/9788 [00:13<00:09, 510.66it/s] 51%|█████     | 4945/9788 [00:13<00:09, 518.78it/s] 51%|█████     | 5009/9788 [00:13<00:09, 523.96it/s] 52%|█████▏    | 5073/9788 [00:13<00:08, 527.48it/s] 52%|█████▏    | 5137/9788 [00:13<00:08, 531.02it/s] 53%|█████▎    | 5201/9788 [00:13<00:08, 531.59it/s] 54%|█████▍    | 5265/9788 [00:13<00:08, 533.45it/s] 54%|█████▍    | 5329/9788 [00:14<00:08, 535.58it/s] 55%|█████▌    | 5393/9788 [00:14<00:08, 538.72it/s] 56%|█████▌    | 5457/9788 [00:14<00:08, 540.80it/s] 56%|█████▋    | 5521/9788 [00:14<00:07, 543.25it/s] 57%|█████▋    | 5585/9788 [00:14<00:07, 543.62it/s] 58%|█████▊    | 5649/9788 [00:14<00:07, 545.55it/s] 58%|█████▊    | 5713/9788 [00:14<00:07, 546.72it/s] 59%|█████▉    | 5777/9788 [00:14<00:07, 548.66it/s] 60%|█████▉    | 5841/9788 [00:15<00:07, 550.73it/s] 60%|██████    | 5905/9788 [00:15<00:07, 552.81it/s] 61%|██████    | 5969/9788 [00:15<00:06, 554.22it/s] 62%|██████▏   | 6033/9788 [00:15<00:06, 557.67it/s] 62%|██████▏   | 6097/9788 [00:15<00:06, 566.20it/s] 63%|██████▎   | 6161/9788 [00:15<00:06, 547.37it/s] 64%|██████▎   | 6225/9788 [00:15<00:06, 542.67it/s] 64%|██████▍   | 6289/9788 [00:15<00:06, 555.14it/s] 65%|██████▍   | 6353/9788 [00:15<00:06, 564.57it/s] 66%|██████▌   | 6417/9788 [00:16<00:05, 571.81it/s] 66%|██████▌   | 6481/9788 [00:16<00:05, 576.93it/s] 67%|██████▋   | 6545/9788 [00:16<00:05, 580.26it/s] 68%|██████▊   | 6609/9788 [00:16<00:05, 582.28it/s] 68%|██████▊   | 6673/9788 [00:16<00:05, 584.53it/s] 69%|██████▉   | 6737/9788 [00:16<00:05, 584.34it/s] 69%|██████▉   | 6801/9788 [00:16<00:05, 584.82it/s] 70%|███████   | 6865/9788 [00:16<00:04, 585.58it/s] 71%|███████   | 6929/9788 [00:16<00:04, 585.78it/s] 71%|███████▏  | 6993/9788 [00:17<00:04, 585.34it/s] 72%|███████▏  | 7057/9788 [00:17<00:04, 585.85it/s] 73%|███████▎  | 7121/9788 [00:17<00:04, 585.67it/s] 73%|███████▎  | 7185/9788 [00:17<00:04, 585.05it/s] 74%|███████▍  | 7249/9788 [00:17<00:04, 585.72it/s] 75%|███████▍  | 7313/9788 [00:17<00:04, 585.13it/s] 75%|███████▌  | 7377/9788 [00:17<00:04, 583.20it/s] 76%|███████▌  | 7441/9788 [00:17<00:04, 583.08it/s] 77%|███████▋  | 7505/9788 [00:17<00:03, 584.78it/s] 77%|███████▋  | 7569/9788 [00:17<00:03, 585.08it/s] 78%|███████▊  | 7633/9788 [00:18<00:03, 585.67it/s] 79%|███████▊  | 7697/9788 [00:18<00:03, 586.67it/s] 79%|███████▉  | 7761/9788 [00:18<00:03, 587.51it/s] 80%|███████▉  | 7825/9788 [00:18<00:03, 588.11it/s] 81%|████████  | 7889/9788 [00:18<00:03, 588.97it/s] 81%|████████▏ | 7953/9788 [00:18<00:03, 589.10it/s] 82%|████████▏ | 8017/9788 [00:18<00:03, 590.25it/s] 83%|████████▎ | 8081/9788 [00:18<00:02, 589.21it/s] 83%|████████▎ | 8145/9788 [00:18<00:02, 584.24it/s] 84%|████████▍ | 8209/9788 [00:19<00:02, 586.82it/s] 85%|████████▍ | 8273/9788 [00:19<00:02, 588.38it/s] 85%|████████▌ | 8337/9788 [00:19<00:02, 591.25it/s] 86%|████████▌ | 8401/9788 [00:19<00:02, 592.59it/s] 86%|████████▋ | 8465/9788 [00:19<00:02, 595.18it/s] 87%|████████▋ | 8529/9788 [00:19<00:02, 596.17it/s] 88%|████████▊ | 8593/9788 [00:19<00:01, 597.52it/s] 88%|████████▊ | 8657/9788 [00:19<00:01, 595.92it/s] 89%|████████▉ | 8721/9788 [00:19<00:01, 596.77it/s] 90%|████████▉ | 8785/9788 [00:20<00:01, 596.25it/s] 90%|█████████ | 8849/9788 [00:20<00:01, 596.98it/s] 91%|█████████ | 8913/9788 [00:20<00:01, 597.00it/s] 92%|█████████▏| 8977/9788 [00:20<00:01, 596.78it/s] 92%|█████████▏| 9041/9788 [00:20<00:01, 597.47it/s] 93%|█████████▎| 9105/9788 [00:20<00:01, 596.70it/s] 94%|█████████▎| 9169/9788 [00:20<00:01, 597.18it/s] 94%|█████████▍| 9233/9788 [00:20<00:00, 595.06it/s] 95%|█████████▍| 9297/9788 [00:20<00:00, 595.97it/s] 96%|█████████▌| 9361/9788 [00:21<00:00, 590.37it/s] 96%|█████████▋| 9425/9788 [00:21<00:00, 592.74it/s] 97%|█████████▋| 9489/9788 [00:21<00:00, 594.45it/s] 98%|█████████▊| 9553/9788 [00:21<00:00, 595.47it/s] 98%|█████████▊| 9617/9788 [00:21<00:00, 595.73it/s] 99%|█████████▉| 9681/9788 [00:21<00:00, 595.99it/s]100%|█████████▉| 9745/9788 [00:21<00:00, 596.41it/s]100%|██████████| 9788/9788 [00:21<00:00, 450.84it/s]
2024-03-13:16:38:54,863 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
2024-03-13:16:38:54,863 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
2024-03-13:16:38:54,863 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
2024-03-13:16:38:54,863 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
2024-03-13:16:38:54,863 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
2024-03-13:16:38:54,863 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
2024-03-13:16:38:54,863 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
2024-03-13:16:38:54,863 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:01,  6.79it/s] 25%|██▌       | 2/8 [00:00<00:01,  3.88it/s] 38%|███▊      | 3/8 [00:00<00:01,  3.95it/s] 50%|█████     | 4/8 [00:00<00:00,  4.14it/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.92it/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.05it/s]100%|██████████| 8/8 [00:01<00:00,  5.55it/s]100%|██████████| 8/8 [00:01<00:00,  4.74it/s]
bootstrapping for stddev: perplexity
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<01:38,  1.00it/s]  2%|▏         | 2/100 [00:01<00:59,  1.64it/s] 13%|█▎        | 13/100 [00:01<00:06, 13.41it/s] 60%|██████    | 60/100 [00:01<00:00, 69.47it/s]100%|██████████| 100/100 [00:01<00:00, 56.29it/s]
hf (pretrained=lomahony/pythia-1b-helpful-sfted1-ppo-3epochs,revision=main-epoch1-1800), gen_kwargs: (None), limit: None, num_fewshot: 0, batch_size: 16
|    Tasks     |Version|Filter|n-shot|    Metric     |   Value    |   |  Stderr   |
|--------------|------:|------|-----:|---------------|-----------:|---|-----------|
|arc_challenge |      1|none  |     0|acc            |      0.2176|±  |     0.0121|
|              |       |none  |     0|acc_norm       |      0.2611|±  |     0.0128|
|arc_easy      |      1|none  |     0|acc            |      0.2656|±  |     0.0091|
|              |       |none  |     0|acc_norm       |      0.2635|±  |     0.0090|
|boolq         |      2|none  |     0|acc            |      0.6211|±  |     0.0085|
|hellaswag     |      1|none  |     0|acc            |      0.2548|±  |     0.0043|
|              |       |none  |     0|acc_norm       |      0.2650|±  |     0.0044|
|lambada_openai|      1|none  |     0|perplexity     |4793806.0753|±  |482414.9081|
|              |       |none  |     0|acc            |      0.0000|±  |          0|
|openbookqa    |      1|none  |     0|acc            |      0.1800|±  |     0.0172|
|              |       |none  |     0|acc_norm       |      0.3060|±  |     0.0206|
|piqa          |      1|none  |     0|acc            |      0.5337|±  |     0.0116|
|              |       |none  |     0|acc_norm       |      0.5114|±  |     0.0117|
|sciq          |      1|none  |     0|acc            |      0.1930|±  |     0.0125|
|              |       |none  |     0|acc_norm       |      0.2110|±  |     0.0129|
|wikitext      |      2|none  |     0|word_perplexity| 404483.1102|±  |N/A        |
|              |       |none  |     0|byte_perplexity|     11.1820|±  |N/A        |
|              |       |none  |     0|bits_per_byte  |      3.4831|±  |N/A        |
|winogrande    |      1|none  |     0|acc            |      0.5020|±  |     0.0141|

