The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `8`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-01-16:00:35:22,918 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-01-16:00:35:22,918 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-01-16:00:35:22,918 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-01-16:00:35:22,918 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-01-16:00:35:22,918 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-01-16:00:35:22,918 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-01-16:00:35:22,918 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-01-16:00:35:22,919 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-01-16:00:35:22,919 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-01-16:00:35:22,919 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-01-16:00:35:22,919 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-01-16:00:35:22,919 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-01-16:00:35:22,919 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-01-16:00:35:22,919 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-01-16:00:35:22,919 INFO     [utils.py:145] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-01-16:00:35:22,919 INFO     [utils.py:148] Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-01-16:00:35:25,009 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-01-16:00:35:25,011 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-01-16:00:35:25,012 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-01-16:00:35:25,012 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-01-16:00:35:25,012 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-01-16:00:35:25,012 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-01-16:00:35:25,012 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-01-16:00:35:25,013 INFO     [config.py:58] PyTorch version 2.1.2 available.
2024-01-16:00:35:36,896 INFO     [__main__.py:156] Verbosity set to INFO
2024-01-16:00:35:36,896 INFO     [__main__.py:156] Verbosity set to INFO
2024-01-16:00:35:36,896 INFO     [__main__.py:156] Verbosity set to INFO
2024-01-16:00:35:36,896 INFO     [__main__.py:156] Verbosity set to INFO
2024-01-16:00:35:36,897 INFO     [__main__.py:156] Verbosity set to INFO
2024-01-16:00:35:36,897 INFO     [__main__.py:156] Verbosity set to INFO
2024-01-16:00:35:36,897 INFO     [__main__.py:156] Verbosity set to INFO
2024-01-16:00:35:36,897 INFO     [__main__.py:156] Verbosity set to INFO
2024-01-16:00:35:52,991 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-01-16:00:35:52,991 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-01-16:00:35:52,991 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-01-16:00:35:52,991 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-01-16:00:35:52,991 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-01-16:00:35:52,992 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-01-16:00:35:52,991 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-01-16:00:35:52,993 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-01-16:00:36:48,544 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-01-16:00:36:48,550 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
2024-01-16:00:36:48,814 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-01-16:00:36:48,820 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
2024-01-16:00:36:49,104 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-01-16:00:36:49,109 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
2024-01-16:00:36:49,245 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-01-16:00:36:49,249 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
2024-01-16:00:36:49,696 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-01-16:00:36:49,700 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
2024-01-16:00:36:49,812 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-01-16:00:36:49,816 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
2024-01-16:00:36:50,587 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-01-16:00:36:50,591 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
2024-01-16:00:36:50,679 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-01-16:00:36:50,683 INFO     [__main__.py:229] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'sciq', 'wikitext', 'winogrande']
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-01-16:00:36:54,079 INFO     [huggingface.py:298] Using 8 devices with data parallelism
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-01-16:00:37:00,614 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-01-16:00:37:00,614 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
2024-01-16:00:37:00,690 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-01-16:00:37:00,690 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
2024-01-16:00:37:00,775 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-01-16:00:37:00,775 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
2024-01-16:00:37:00,986 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-01-16:00:37:00,986 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-01-16:00:37:01,584 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-01-16:00:37:01,584 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-01-16:00:37:01,853 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-01-16:00:37:01,854 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
2024-01-16:00:37:01,854 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-01-16:00:37:01,854 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-01-16:00:37:02,174 WARNING  [task.py:600] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
2024-01-16:00:37:02,175 WARNING  [task.py:612] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/admin/home-laura/venvs/venv-lm-evaluation-harness/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-01-16:00:37:10,201 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:10,201 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:10,237 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:10,237 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:10,465 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:10,465 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:10,607 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:10,607 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:10,916 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:10,916 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:10,925 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:10,925 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:11,928 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:11,928 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:12,158 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:12,158 WARNING  [task.py:284] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-01-16:00:37:21,305 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:21,305 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:21,305 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:21,305 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:21,305 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-01-16:00:37:21,305 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:21,391 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:21,391 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:21,391 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:21,391 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:21,391 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-01-16:00:37:21,391 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:21,467 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:21,468 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:21,468 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:21,468 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:21,468 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-01-16:00:37:21,468 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:21,520 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:21,521 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:21,521 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:21,521 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:21,521 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-01-16:00:37:21,521 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:21,850 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:21,850 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:21,850 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:21,850 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:21,850 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-01-16:00:37:21,850 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:22,553 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:22,554 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:22,554 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:22,554 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:22,554 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-01-16:00:37:22,554 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:23,290 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:23,290 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:23,290 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:23,291 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:23,291 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-01-16:00:37:23,291 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:23,464 WARNING  [task.py:600] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:23,464 WARNING  [task.py:612] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:23,464 WARNING  [task.py:600] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-01-16:00:37:23,464 WARNING  [task.py:612] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:23,464 WARNING  [task.py:600] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-01-16:00:37:23,464 WARNING  [task.py:612] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
2024-01-16:00:37:26,064 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-01-16:00:37:26,065 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-01-16:00:37:26,065 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-01-16:00:37:26,065 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-01-16:00:37:26,065 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-01-16:00:37:26,065 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-01-16:00:37:26,065 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-01-16:00:37:26,065 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-01-16:00:37:26,065 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-01-16:00:37:26,065 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-01-16:00:37:26,065 INFO     [task.py:337] Building contexts for task on rank 2...
2024-01-16:00:37:26,079 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-01-16:00:37:26,080 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-01-16:00:37:26,080 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-01-16:00:37:26,080 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-01-16:00:37:26,080 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-01-16:00:37:26,080 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-01-16:00:37:26,080 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-01-16:00:37:26,080 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-01-16:00:37:26,080 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-01-16:00:37:26,080 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-01-16:00:37:26,080 INFO     [task.py:337] Building contexts for task on rank 6...
2024-01-16:00:37:26,128 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-01-16:00:37:26,128 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-01-16:00:37:26,128 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-01-16:00:37:26,128 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-01-16:00:37:26,128 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-01-16:00:37:26,128 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-01-16:00:37:26,128 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-01-16:00:37:26,128 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-01-16:00:37:26,128 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-01-16:00:37:26,128 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-01-16:00:37:26,128 INFO     [task.py:337] Building contexts for task on rank 5...
2024-01-16:00:37:26,500 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-01-16:00:37:26,501 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-01-16:00:37:26,501 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-01-16:00:37:26,501 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-01-16:00:37:26,501 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-01-16:00:37:26,501 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-01-16:00:37:26,501 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-01-16:00:37:26,501 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-01-16:00:37:26,501 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-01-16:00:37:26,501 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-01-16:00:37:26,501 INFO     [task.py:337] Building contexts for task on rank 7...
2024-01-16:00:37:26,643 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-01-16:00:37:26,643 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-01-16:00:37:26,643 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-01-16:00:37:26,643 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-01-16:00:37:26,643 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-01-16:00:37:26,643 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-01-16:00:37:26,643 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-01-16:00:37:26,643 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-01-16:00:37:26,643 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-01-16:00:37:26,643 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-01-16:00:37:26,644 INFO     [task.py:337] Building contexts for task on rank 4...
2024-01-16:00:37:27,037 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-01-16:00:37:27,038 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-01-16:00:37:27,038 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-01-16:00:37:27,038 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-01-16:00:37:27,038 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-01-16:00:37:27,038 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-01-16:00:37:27,038 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-01-16:00:37:27,038 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-01-16:00:37:27,038 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-01-16:00:37:27,038 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-01-16:00:37:27,038 INFO     [task.py:337] Building contexts for task on rank 0...
2024-01-16:00:37:27,770 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-01-16:00:37:27,770 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-01-16:00:37:27,770 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-01-16:00:37:27,770 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-01-16:00:37:27,770 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-01-16:00:37:27,770 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-01-16:00:37:27,770 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-01-16:00:37:27,770 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-01-16:00:37:27,770 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-01-16:00:37:27,770 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-01-16:00:37:27,771 INFO     [task.py:337] Building contexts for task on rank 3...
2024-01-16:00:37:28,100 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_challenge from None to 0
2024-01-16:00:37:28,100 WARNING  [evaluator.py:141] Overwriting default num_fewshot of arc_easy from None to 0
2024-01-16:00:37:28,100 WARNING  [evaluator.py:141] Overwriting default num_fewshot of boolq from None to 0
2024-01-16:00:37:28,100 WARNING  [evaluator.py:141] Overwriting default num_fewshot of hellaswag from None to 0
2024-01-16:00:37:28,100 WARNING  [evaluator.py:141] Overwriting default num_fewshot of lambada_openai from None to 0
2024-01-16:00:37:28,100 WARNING  [evaluator.py:141] Overwriting default num_fewshot of openbookqa from None to 0
2024-01-16:00:37:28,100 WARNING  [evaluator.py:141] Overwriting default num_fewshot of piqa from None to 0
2024-01-16:00:37:28,100 WARNING  [evaluator.py:141] Overwriting default num_fewshot of sciq from None to 0
2024-01-16:00:37:28,100 WARNING  [evaluator.py:141] Overwriting default num_fewshot of wikitext from None to 0
2024-01-16:00:37:28,100 WARNING  [evaluator.py:141] Overwriting default num_fewshot of winogrande from None to 0
2024-01-16:00:37:28,101 INFO     [task.py:337] Building contexts for task on rank 1...
2024-01-16:00:37:39,826 INFO     [task.py:337] Building contexts for task on rank 2...
2024-01-16:00:37:39,826 INFO     [task.py:337] Building contexts for task on rank 0...
2024-01-16:00:37:39,826 INFO     [task.py:337] Building contexts for task on rank 5...
2024-01-16:00:37:39,826 INFO     [task.py:337] Building contexts for task on rank 7...
2024-01-16:00:37:39,826 INFO     [task.py:337] Building contexts for task on rank 6...
2024-01-16:00:37:39,826 INFO     [task.py:337] Building contexts for task on rank 3...
2024-01-16:00:37:39,826 INFO     [task.py:337] Building contexts for task on rank 4...
2024-01-16:00:37:39,826 INFO     [task.py:337] Building contexts for task on rank 1...
2024-01-16:00:37:40,239 INFO     [task.py:337] Building contexts for task on rank 3...
2024-01-16:00:37:40,239 INFO     [task.py:337] Building contexts for task on rank 7...
2024-01-16:00:37:40,239 INFO     [task.py:337] Building contexts for task on rank 0...
2024-01-16:00:37:40,239 INFO     [task.py:337] Building contexts for task on rank 2...
2024-01-16:00:37:40,239 INFO     [task.py:337] Building contexts for task on rank 5...
2024-01-16:00:37:40,240 INFO     [task.py:337] Building contexts for task on rank 1...
2024-01-16:00:37:40,240 INFO     [task.py:337] Building contexts for task on rank 4...
2024-01-16:00:37:40,240 INFO     [task.py:337] Building contexts for task on rank 6...
2024-01-16:00:37:40,581 INFO     [task.py:337] Building contexts for task on rank 0...
2024-01-16:00:37:40,581 INFO     [task.py:337] Building contexts for task on rank 7...
2024-01-16:00:37:40,581 INFO     [task.py:337] Building contexts for task on rank 2...
2024-01-16:00:37:40,581 INFO     [task.py:337] Building contexts for task on rank 5...
2024-01-16:00:37:40,581 INFO     [task.py:337] Building contexts for task on rank 3...
2024-01-16:00:37:40,581 INFO     [task.py:337] Building contexts for task on rank 6...
2024-01-16:00:37:40,582 INFO     [task.py:337] Building contexts for task on rank 1...
2024-01-16:00:37:40,582 INFO     [task.py:337] Building contexts for task on rank 4...
2024-01-16:00:37:42,108 INFO     [task.py:337] Building contexts for task on rank 1...
2024-01-16:00:37:42,108 INFO     [task.py:337] Building contexts for task on rank 3...
2024-01-16:00:37:42,108 INFO     [task.py:337] Building contexts for task on rank 2...
2024-01-16:00:37:42,108 INFO     [task.py:337] Building contexts for task on rank 0...
2024-01-16:00:37:42,108 INFO     [task.py:337] Building contexts for task on rank 7...
2024-01-16:00:37:42,108 INFO     [task.py:337] Building contexts for task on rank 6...
2024-01-16:00:37:42,108 INFO     [task.py:337] Building contexts for task on rank 5...
2024-01-16:00:37:42,108 INFO     [task.py:337] Building contexts for task on rank 4...
2024-01-16:00:37:43,237 INFO     [task.py:337] Building contexts for task on rank 7...
2024-01-16:00:37:43,237 INFO     [task.py:337] Building contexts for task on rank 3...
2024-01-16:00:37:43,237 INFO     [task.py:337] Building contexts for task on rank 1...
2024-01-16:00:37:43,237 INFO     [task.py:337] Building contexts for task on rank 6...
2024-01-16:00:37:43,237 INFO     [task.py:337] Building contexts for task on rank 5...
2024-01-16:00:37:43,237 INFO     [task.py:337] Building contexts for task on rank 2...
2024-01-16:00:37:43,237 INFO     [task.py:337] Building contexts for task on rank 0...
2024-01-16:00:37:43,237 INFO     [task.py:337] Building contexts for task on rank 4...
2024-01-16:00:37:43,303 INFO     [task.py:337] Building contexts for task on rank 7...
2024-01-16:00:37:43,303 INFO     [task.py:337] Building contexts for task on rank 1...
2024-01-16:00:37:43,303 INFO     [task.py:337] Building contexts for task on rank 5...
2024-01-16:00:37:43,303 INFO     [task.py:337] Building contexts for task on rank 2...
2024-01-16:00:37:43,303 INFO     [task.py:337] Building contexts for task on rank 3...
2024-01-16:00:37:43,303 INFO     [task.py:337] Building contexts for task on rank 0...
2024-01-16:00:37:43,303 INFO     [task.py:337] Building contexts for task on rank 4...
2024-01-16:00:37:43,303 INFO     [task.py:337] Building contexts for task on rank 6...
2024-01-16:00:37:43,601 INFO     [task.py:337] Building contexts for task on rank 7...
2024-01-16:00:37:43,601 INFO     [task.py:337] Building contexts for task on rank 3...
2024-01-16:00:37:43,601 INFO     [task.py:337] Building contexts for task on rank 0...
2024-01-16:00:37:43,601 INFO     [task.py:337] Building contexts for task on rank 4...
2024-01-16:00:37:43,601 INFO     [task.py:337] Building contexts for task on rank 1...
2024-01-16:00:37:43,601 INFO     [task.py:337] Building contexts for task on rank 6...
2024-01-16:00:37:43,601 INFO     [task.py:337] Building contexts for task on rank 2...
2024-01-16:00:37:43,601 INFO     [task.py:337] Building contexts for task on rank 5...
2024-01-16:00:37:43,798 INFO     [task.py:337] Building contexts for task on rank 1...
2024-01-16:00:37:43,798 INFO     [task.py:337] Building contexts for task on rank 3...
2024-01-16:00:37:43,798 INFO     [task.py:337] Building contexts for task on rank 7...
2024-01-16:00:37:43,798 INFO     [task.py:337] Building contexts for task on rank 0...
2024-01-16:00:37:43,798 INFO     [task.py:337] Building contexts for task on rank 5...
2024-01-16:00:37:43,798 INFO     [task.py:337] Building contexts for task on rank 2...
2024-01-16:00:37:43,798 INFO     [task.py:337] Building contexts for task on rank 4...
2024-01-16:00:37:43,798 INFO     [task.py:337] Building contexts for task on rank 6...
2024-01-16:00:37:43,827 INFO     [task.py:337] Building contexts for task on rank 3...
2024-01-16:00:37:43,827 INFO     [task.py:337] Building contexts for task on rank 6...
2024-01-16:00:37:43,827 INFO     [task.py:337] Building contexts for task on rank 1...
2024-01-16:00:37:43,827 INFO     [task.py:337] Building contexts for task on rank 7...
2024-01-16:00:37:43,827 INFO     [task.py:337] Building contexts for task on rank 2...
2024-01-16:00:37:43,827 INFO     [task.py:337] Building contexts for task on rank 5...
2024-01-16:00:37:43,827 INFO     [task.py:337] Building contexts for task on rank 4...
2024-01-16:00:37:43,827 INFO     [task.py:337] Building contexts for task on rank 0...
2024-01-16:00:37:43,871 INFO     [evaluator.py:314] Running loglikelihood requests
2024-01-16:00:37:43,871 INFO     [evaluator.py:314] Running loglikelihood requests
2024-01-16:00:37:43,871 INFO     [evaluator.py:314] Running loglikelihood requests
2024-01-16:00:37:43,871 INFO     [evaluator.py:314] Running loglikelihood requests
2024-01-16:00:37:43,871 INFO     [evaluator.py:314] Running loglikelihood requests
2024-01-16:00:37:43,871 INFO     [evaluator.py:314] Running loglikelihood requests
2024-01-16:00:37:43,871 INFO     [evaluator.py:314] Running loglikelihood requests
2024-01-16:00:37:43,872 INFO     [evaluator.py:314] Running loglikelihood requests
  0%|          | 0/9788 [00:00<?, ?it/s]  0%|          | 1/9788 [00:01<5:25:08,  1.99s/it]  1%|          | 65/9788 [00:02<03:46, 42.92it/s]   1%|         | 145/9788 [00:02<01:29, 107.25it/s]  2%|         | 225/9788 [00:02<00:52, 180.78it/s]  3%|         | 305/9788 [00:02<00:36, 261.57it/s]  4%|         | 385/9788 [00:02<00:27, 344.88it/s]  5%|         | 465/9788 [00:02<00:21, 425.57it/s]  6%|         | 545/9788 [00:02<00:18, 500.36it/s]  6%|         | 625/9788 [00:02<00:16, 565.86it/s]  7%|         | 707/9788 [00:02<00:14, 627.75it/s]  8%|         | 794/9788 [00:03<00:13, 690.19it/s]  9%|         | 875/9788 [00:03<00:12, 722.19it/s] 10%|         | 956/9788 [00:03<00:11, 744.73it/s] 11%|         | 1037/9788 [00:03<00:11, 737.21it/s] 11%|        | 1115/9788 [00:03<00:11, 748.88it/s] 12%|        | 1193/9788 [00:03<00:11, 755.89it/s] 13%|        | 1271/9788 [00:03<00:11, 760.71it/s] 14%|        | 1349/9788 [00:03<00:11, 765.26it/s] 15%|        | 1427/9788 [00:03<00:10, 768.91it/s] 15%|        | 1512/9788 [00:03<00:10, 792.82it/s] 16%|        | 1592/9788 [00:04<00:10, 793.23it/s] 17%|        | 1681/9788 [00:04<00:10, 784.41it/s] 18%|        | 1768/9788 [00:04<00:09, 808.85it/s] 19%|        | 1852/9788 [00:04<00:09, 817.77it/s] 20%|        | 1935/9788 [00:04<00:09, 815.54it/s] 21%|        | 2017/9788 [00:04<00:09, 777.44it/s] 21%|       | 2099/9788 [00:04<00:09, 789.58it/s] 22%|       | 2187/9788 [00:04<00:09, 815.57it/s] 23%|       | 2273/9788 [00:04<00:09, 790.07it/s] 24%|       | 2353/9788 [00:05<00:09, 789.66it/s] 25%|       | 2444/9788 [00:05<00:08, 824.18it/s] 26%|       | 2529/9788 [00:05<00:09, 795.32it/s] 27%|       | 2625/9788 [00:05<00:08, 804.08it/s] 28%|       | 2706/9788 [00:05<00:08, 805.53it/s] 29%|       | 2801/9788 [00:05<00:08, 806.97it/s] 29%|       | 2887/9788 [00:05<00:08, 821.59it/s] 30%|       | 2977/9788 [00:05<00:08, 805.26it/s] 31%|      | 3068/9788 [00:05<00:08, 834.34it/s] 32%|      | 3152/9788 [00:05<00:07, 833.95it/s] 33%|      | 3236/9788 [00:06<00:08, 799.30it/s] 34%|      | 3329/9788 [00:06<00:08, 797.24it/s] 35%|      | 3425/9788 [00:06<00:07, 805.31it/s] 36%|      | 3506/9788 [00:06<00:07, 805.61it/s] 37%|      | 3593/9788 [00:06<00:07, 823.73it/s] 38%|      | 3681/9788 [00:06<00:07, 802.62it/s] 39%|      | 3769/9788 [00:06<00:07, 824.20it/s] 39%|      | 3852/9788 [00:06<00:07, 823.77it/s] 40%|      | 3937/9788 [00:06<00:07, 792.73it/s] 41%|      | 4029/9788 [00:07<00:06, 828.75it/s] 42%|     | 4113/9788 [00:07<00:07, 795.84it/s] 43%|     | 4205/9788 [00:07<00:06, 830.82it/s] 44%|     | 4289/9788 [00:07<00:06, 796.48it/s] 45%|     | 4380/9788 [00:07<00:06, 828.47it/s] 46%|     | 4465/9788 [00:07<00:06, 801.17it/s] 47%|     | 4561/9788 [00:07<00:06, 808.85it/s] 47%|     | 4643/9788 [00:07<00:06, 811.46it/s] 48%|     | 4731/9788 [00:07<00:06, 830.74it/s] 49%|     | 4817/9788 [00:08<00:06, 802.07it/s] 50%|     | 4908/9788 [00:08<00:05, 832.45it/s] 51%|     | 4993/9788 [00:08<00:05, 800.33it/s] 52%|    | 5074/9788 [00:08<00:05, 802.99it/s] 53%|    | 5155/9788 [00:08<00:05, 787.34it/s] 54%|    | 5244/9788 [00:08<00:05, 816.71it/s] 54%|    | 5327/9788 [00:08<00:05, 819.78it/s] 55%|    | 5410/9788 [00:08<00:05, 787.02it/s] 56%|    | 5502/9788 [00:08<00:05, 824.86it/s] 57%|    | 5585/9788 [00:09<00:05, 790.45it/s] 58%|    | 5680/9788 [00:09<00:04, 835.57it/s] 59%|    | 5765/9788 [00:09<00:05, 803.19it/s] 60%|    | 5857/9788 [00:09<00:04, 797.86it/s] 61%|    | 5953/9788 [00:09<00:04, 806.29it/s] 62%|   | 6049/9788 [00:09<00:04, 811.68it/s] 63%|   | 6145/9788 [00:09<00:04, 816.20it/s] 64%|   | 6241/9788 [00:09<00:04, 818.99it/s] 65%|   | 6337/9788 [00:09<00:04, 826.19it/s] 66%|   | 6433/9788 [00:10<00:04, 832.41it/s] 67%|   | 6529/9788 [00:10<00:03, 835.06it/s] 68%|   | 6625/9788 [00:10<00:03, 835.06it/s] 69%|   | 6721/9788 [00:10<00:03, 835.97it/s] 70%|   | 6817/9788 [00:10<00:03, 840.73it/s] 71%|   | 6913/9788 [00:10<00:03, 843.79it/s] 72%|  | 7009/9788 [00:10<00:03, 842.11it/s] 73%|  | 7105/9788 [00:10<00:03, 841.57it/s] 74%|  | 7201/9788 [00:10<00:03, 839.98it/s] 75%|  | 7297/9788 [00:11<00:02, 840.40it/s] 76%|  | 7393/9788 [00:11<00:02, 840.65it/s] 77%|  | 7489/9788 [00:11<00:02, 839.54it/s] 77%|  | 7585/9788 [00:11<00:02, 839.38it/s] 78%|  | 7681/9788 [00:11<00:02, 838.21it/s] 79%|  | 7777/9788 [00:11<00:02, 840.55it/s] 80%|  | 7873/9788 [00:11<00:02, 844.08it/s] 81%| | 7969/9788 [00:11<00:02, 845.15it/s] 82%| | 8065/9788 [00:11<00:02, 843.54it/s] 83%| | 8161/9788 [00:12<00:01, 840.96it/s] 84%| | 8257/9788 [00:12<00:01, 840.37it/s] 85%| | 8353/9788 [00:12<00:01, 838.34it/s] 86%| | 8449/9788 [00:12<00:01, 840.45it/s] 87%| | 8545/9788 [00:12<00:01, 843.19it/s] 88%| | 8641/9788 [00:12<00:01, 843.37it/s] 89%| | 8737/9788 [00:12<00:01, 845.11it/s] 90%| | 8833/9788 [00:12<00:01, 845.05it/s] 91%| | 8929/9788 [00:12<00:01, 845.00it/s] 92%|| 9025/9788 [00:13<00:00, 845.09it/s] 93%|| 9121/9788 [00:13<00:00, 844.52it/s] 94%|| 9217/9788 [00:13<00:00, 842.82it/s] 95%|| 9302/9788 [00:13<00:00, 829.54it/s] 96%|| 9393/9788 [00:13<00:00, 816.53it/s] 97%|| 9489/9788 [00:13<00:00, 822.05it/s] 98%|| 9585/9788 [00:13<00:00, 831.41it/s] 99%|| 9681/9788 [00:13<00:00, 833.72it/s]100%|| 9777/9788 [00:14<00:00, 839.50it/s]100%|| 9788/9788 [00:14<00:00, 698.41it/s]
2024-01-16:00:38:03,245 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
2024-01-16:00:38:03,245 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
2024-01-16:00:38:03,245 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
2024-01-16:00:38:03,245 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
2024-01-16:00:38:03,245 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
2024-01-16:00:38:03,245 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
2024-01-16:00:38:03,245 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
  0%|          | 0/8 [00:00<?, ?it/s]2024-01-16:00:38:03,245 INFO     [evaluator.py:314] Running loglikelihood_rolling requests
 25%|       | 2/8 [00:00<00:00, 12.01it/s] 50%|     | 4/8 [00:00<00:00, 12.66it/s] 75%|  | 6/8 [00:00<00:00, 12.42it/s]100%|| 8/8 [00:00<00:00, 14.49it/s]
bootstrapping for stddev: perplexity
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<01:58,  1.20s/it]  8%|         | 8/100 [00:01<00:11,  7.67it/s]100%|| 100/100 [00:01<00:00, 69.02it/s]
hf (pretrained=EleutherAI/pythia-70m), gen_kwargs: (None), limit: None, num_fewshot: 0, batch_size: 16
|    Tasks     |Version|Filter|n-shot|    Metric     |Value |   |Stderr|
|--------------|------:|------|-----:|---------------|-----:|---|------|
|arc_challenge |      1|none  |     0|acc            |0.2184|  |0.0121|
|              |       |none  |     0|acc_norm       |0.2167|  |0.0120|
|arc_easy      |      1|none  |     0|acc            |0.2508|  |0.0089|
|              |       |none  |     0|acc_norm       |0.2508|  |0.0089|
|boolq         |      2|none  |     0|acc            |0.3878|  |0.0085|
|hellaswag     |      1|none  |     0|acc            |0.2505|  |0.0043|
|              |       |none  |     0|acc_norm       |0.2506|  |0.0043|
|lambada_openai|      1|none  |     0|perplexity     |   NaN|  |NaN   |
|              |       |none  |     0|acc            |0.0089|  |0.0013|
|openbookqa    |      1|none  |     0|acc            |0.2420|  |0.0192|
|              |       |none  |     0|acc_norm       |0.2660|  |0.0198|
|piqa          |      1|none  |     0|acc            |0.5005|  |0.0117|
|              |       |none  |     0|acc_norm       |0.5000|  |0.0117|
|sciq          |      1|none  |     0|acc            |0.0290|  |0.0053|
|              |       |none  |     0|acc_norm       |0.0310|  |0.0055|
|wikitext      |      2|none  |     0|word_perplexity|   NaN|  |N/A   |
|              |       |none  |     0|byte_perplexity|   NaN|  |N/A   |
|              |       |none  |     0|bits_per_byte  |   NaN|  |N/A   |
|winogrande    |      1|none  |     0|acc            |0.4925|  |0.0141|

